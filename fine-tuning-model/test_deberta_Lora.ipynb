{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed986337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26748\\586864222.py:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  replace('\\s+', ' ', regex=True)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(path):\n",
    "    \"\"\" Remove unnecessary characters and encode the sentiment labels.\n",
    "\n",
    "    The type of preprocessing required changes based on the dataset. For the\n",
    "    IMDb dataset, the review texts contains HTML break tags (<br/>) leftover\n",
    "    from the scraping process, and some unnecessary whitespace, which are\n",
    "    removed. Finally, encode the sentiment labels as 0 for \"negative\" and 1 for\n",
    "    \"positive\". This method assumes the dataset file contains the headers\n",
    "    \"review\" and \"sentiment\".\n",
    "\n",
    "    Parameters:\n",
    "        path (str): A path to a dataset file containing the sentiment analysis\n",
    "            dataset. The structure of the file should be as follows: one column\n",
    "            called \"review\" containing the review text, and one column called\n",
    "            \"sentiment\" containing the ground truth label. The label options\n",
    "            should be \"negative\" and \"positive\".\n",
    "\n",
    "    Returns:\n",
    "        df_dataset (pd.DataFrame): A DataFrame containing the raw data\n",
    "            loaded from the self.dataset path. In addition to the expected\n",
    "            \"review\" and \"sentiment\" columns, are:\n",
    "\n",
    "            > review_cleaned - a copy of the \"review\" column with the HTML\n",
    "                break tags and unnecessary whitespace removed\n",
    "\n",
    "            > sentiment_encoded - a copy of the \"sentiment\" column with the\n",
    "                \"negative\" values mapped to 0 and \"positive\" values mapped\n",
    "                to 1\n",
    "    \"\"\"\n",
    "    df_dataset = pd.read_csv(path)\n",
    "\n",
    "    df_dataset['review_cleaned'] = df_dataset['review'].\\\n",
    "        apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "    df_dataset['review_cleaned'] = df_dataset['review_cleaned'].\\\n",
    "        replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    df_dataset['sentiment_encoded'] = df_dataset['sentiment'].\\\n",
    "        apply(lambda x: 0 if x == 'negative' else 1)\n",
    "\n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "5  Probably my all-time favorite movie, a story o...  positive   \n",
      "6  I sure would like to see a resurrection of a u...  positive   \n",
      "7  This show was an amazing, fresh & innovative i...  negative   \n",
      "8  Encouraged by the positive comments about this...  negative   \n",
      "9  If you like original gut wrenching laughter yo...  positive   \n",
      "\n",
      "                                      review_cleaned  sentiment_encoded  \n",
      "0  One of the other reviewers has mentioned that ...                  1  \n",
      "1  A wonderful little production. The filming tec...                  1  \n",
      "2  I thought this was a wonderful way to spend ti...                  1  \n",
      "3  Basically there's a family where a little boy ...                  0  \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...                  1  \n",
      "5  Probably my all-time favorite movie, a story o...                  1  \n",
      "6  I sure would like to see a resurrection of a u...                  1  \n",
      "7  This show was an amazing, fresh & innovative i...                  0  \n",
      "8  Encouraged by the positive comments about this...                  0  \n",
      "9  If you like original gut wrenching laughter yo...                  1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = preprocess_dataset(\"C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv\")\n",
    "\n",
    "print(dataset.head(10))  # ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "\n",
    "\n",
    "class FineTuningPipeline:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function = nn.CrossEntropyLoss(),\n",
    "            val_size = 0.1,\n",
    "            epochs = 4,\n",
    "            seed = 42):\n",
    "\n",
    "        self.df_dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.val_size = val_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "        # Check if GPU is available for faster training time\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        # Perform fine-tuning\n",
    "        self.model.to(self.device)\n",
    "        self.set_seeds()\n",
    "        self.token_ids, self.attention_masks = self.tokenize_dataset()\n",
    "        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        self.fine_tune()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Tokenize input text and return the token IDs and attention mask.\n",
    "\n",
    "        Tokenize an input string, setting a maximum length of 512 tokens.\n",
    "        Sequences with more than 512 tokens will be truncated to this limit,\n",
    "        and sequences with less than 512 tokens will be supplemented with [PAD]\n",
    "        tokens to bring them up to this limit. The datatype of the returned\n",
    "        tensors will be the PyTorch tensor format. These return values are\n",
    "        tensors of size 1 x max_length where max_length is the maximum number\n",
    "        of tokens per input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs for each token in\n",
    "                the input sequence.\n",
    "\n",
    "            attention_mask (torch.Tensor): A tensor of 1s and 0s where a 1\n",
    "                indicates a token can be attended to during the attention\n",
    "                process, and a 0 indicates a token should be ignored. This is\n",
    "                used to prevent BERT from attending to [PAD] tokens during its\n",
    "                training/inference.\n",
    "        \"\"\"\n",
    "        batch_encoder = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length = 128,\n",
    "            #max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt')\n",
    "\n",
    "        token_ids = batch_encoder['input_ids']\n",
    "        attention_mask = batch_encoder['attention_mask']\n",
    "\n",
    "        return token_ids, attention_mask\n",
    "\n",
    "    def tokenize_dataset(self):\n",
    "        \"\"\" Apply the self.tokenize method to the fine-tuning dataset.\n",
    "\n",
    "        Tokenize and return the input sequence for each row in the fine-tuning\n",
    "        dataset given by self.dataset. The return values are tensors of size\n",
    "        len_dataset x max_length where len_dataset is the number of rows in the\n",
    "        fine-tuning dataset and max_length is the maximum number of tokens per\n",
    "        input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of tensors containing token IDs\n",
    "            for each token in the input sequence.\n",
    "\n",
    "            attention_masks (torch.Tensor): A tensor of tensors containing the\n",
    "                attention masks for each sequence in the fine-tuning dataset.\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for review in self.df_dataset['review_cleaned']:\n",
    "            tokens, masks = self.tokenize(review)\n",
    "            token_ids.append(tokens)\n",
    "            attention_masks.append(masks)\n",
    "\n",
    "        token_ids = torch.cat(token_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return token_ids, attention_masks\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        \"\"\" Create dataloaders for the train and validation set.\n",
    "\n",
    "        Split the tokenized dataset into train and validation sets according to\n",
    "        the self.val_size value. For example, if self.val_size is set to 0.1,\n",
    "        90% of the data will be used to form the train set, and 10% for the\n",
    "        validation set. Convert the \"sentiment_encoded\" column (labels for each\n",
    "        row) to PyTorch tensors to be used in the dataloaders.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            train_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the train data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "            val_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the validation data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "        \"\"\"\n",
    "        train_ids, val_ids = train_test_split(\n",
    "                        self.token_ids,\n",
    "                        test_size=self.val_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "        train_masks, val_masks = train_test_split(\n",
    "                                    self.attention_masks,\n",
    "                                    test_size=self.val_size,\n",
    "                                    shuffle=False)\n",
    "\n",
    "        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)\n",
    "        train_labels, val_labels = train_test_split(\n",
    "                                        labels,\n",
    "                                        test_size=self.val_size,\n",
    "                                        shuffle=False)\n",
    "\n",
    "        train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=32) # batch_size = 16 before\n",
    "        val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=32) # batch_size = 16 before\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        \"\"\" Create a linear scheduler for the learning rate.\n",
    "\n",
    "        Create a scheduler with a learning rate that increases linearly from 0\n",
    "        to a maximum value (called the warmup period), then decreases linearly\n",
    "        to 0 again. num_warmup_steps is set to 0 here based on an example from\n",
    "        Hugging Face:\n",
    "\n",
    "        https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2\n",
    "        d008813037968a9e58/examples/run_glue.py#L308\n",
    "\n",
    "        Read more about schedulers here:\n",
    "\n",
    "        https://huggingface.co/docs/transformers/main_classes/optimizer_\n",
    "        schedules#transformers.get_linear_schedule_with_warmup\n",
    "        \"\"\"\n",
    "        num_training_steps = self.epochs * len(self.train_dataloader)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps)\n",
    "\n",
    "        return scheduler\n",
    "\n",
    "    def set_seeds(self):\n",
    "        \"\"\" Set the random seeds so that results are reproduceable.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        \"\"\"Train the classification head on the BERT model.\n",
    "\n",
    "        Fine-tune the model by training the classification head (linear layer)\n",
    "        sitting on top of the BERT model. The model trained on the data in the\n",
    "        self.train_dataloader, and validated at the end of each epoch on the\n",
    "        data in the self.val_dataloader. The series of steps are described\n",
    "        below:\n",
    "\n",
    "        Training:\n",
    "\n",
    "        > Create a dictionary to store the average training loss and average\n",
    "          validation loss for each epoch.\n",
    "        > Store the time at the start of training, this is used to calculate\n",
    "          the time taken for the entire training process.\n",
    "        > Begin a loop to train the model for each epoch in self.epochs.\n",
    "\n",
    "        For each epoch:\n",
    "\n",
    "        > Switch the model to train mode. This will cause the model to behave\n",
    "          differently than when in evaluation mode (e.g. the batchnorm and\n",
    "          dropout layers are activated in train mode, but disabled in\n",
    "          evaluation mode).\n",
    "        > Set the training loss to 0 for the start of the epoch. This is used\n",
    "          to track the loss of the model on the training data over subsequent\n",
    "          epochs. The loss should decrease with each epoch if training is\n",
    "          successful.\n",
    "        > Store the time at the start of the epoch, this is used to calculate\n",
    "          the time taken for the epoch to be completed.\n",
    "        > As per the BERT authors' recommendations, the training data for each\n",
    "          epoch is split into batches. Loop through the training process for\n",
    "          each batch.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the zero_grad method to reset the calculated gradients from\n",
    "          the previous iteration of this loop.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Increment the total loss for the epoch. The loss is returned from the\n",
    "          model as a PyTorch tensor so extract the float value using the item\n",
    "          method.\n",
    "        > Perform a backward pass of the model and propagate the loss through\n",
    "          the classifier head. This will allow the model to determine what\n",
    "          adjustments to make to the weights and biases to improve its\n",
    "          performance on the batch.\n",
    "        > Clip the gradients to be no larger than 1.0 so the model does not\n",
    "          suffer from the exploding gradients problem.\n",
    "        > Call the optimizer to take a step in the direction of the error\n",
    "          surface as determined by the backward pass.\n",
    "\n",
    "        After training on each batch:\n",
    "\n",
    "        > Calculate the average loss and time taken for training on the epoch.\n",
    "\n",
    "        Validation step for the epoch:\n",
    "\n",
    "        > Switch the model to evaluation mode.\n",
    "        > Set the validation loss to 0. This is used to track the loss of the\n",
    "          model on the validation data over subsequent epochs. The loss should\n",
    "          decrease with each epoch if training was successful.\n",
    "        > Store the time at the start of the validation, this is used to\n",
    "          calculate the time taken for the validation for this epoch to be\n",
    "          completed.\n",
    "        > Split the validation data into batches.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the no_grad method to instruct the model not to calculate the\n",
    "          gradients since we wil not be performing any optimization steps here,\n",
    "          only inference.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Extract the logits and labels from the model and move them to the CPU\n",
    "          (if they are not already there).\n",
    "        > Increment the loss and calculate the accuracy based on the true\n",
    "          labels in the validation dataloader.\n",
    "        > Calculate the average loss and accuracy, and add these to the loss\n",
    "          dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = {\n",
    "            'epoch': [i+1 for i in range(self.epochs)],\n",
    "            'average training loss': [],\n",
    "            'average validation loss': []\n",
    "        }\n",
    "\n",
    "        t0_train = datetime.now()\n",
    "\n",
    "        for epoch in range(0, self.epochs):\n",
    "\n",
    "            # Train step\n",
    "            self.model.train()\n",
    "            training_loss = 0\n",
    "            t0_epoch = datetime.now()\n",
    "\n",
    "            print(f'{\"-\"*20} Epoch {epoch+1} {\"-\"*20}')\n",
    "            print('\\nTraining:\\n---------')\n",
    "            print(f'Start Time:       {t0_epoch}')\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                loss, logits = self.model(\n",
    "                    batch_token_ids,\n",
    "                    token_type_ids = None,\n",
    "                    attention_mask=batch_attention_mask,\n",
    "                    labels=batch_labels,\n",
    "                    return_dict=False)\n",
    "\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            average_train_loss = training_loss / len(self.train_dataloader)\n",
    "            time_epoch = datetime.now() - t0_epoch\n",
    "\n",
    "            print(f'Average Loss:     {average_train_loss}')\n",
    "            print(f'Time Taken:       {time_epoch}')\n",
    "\n",
    "            # Validation step\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            t0_val = datetime.now()\n",
    "\n",
    "            print('\\nValidation:\\n---------')\n",
    "            print(f'Start Time:       {t0_val}')\n",
    "\n",
    "            for batch in self.val_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    (loss, logits) = self.model(\n",
    "                        batch_token_ids,\n",
    "                        attention_mask = batch_attention_mask,\n",
    "                        labels = batch_labels,\n",
    "                        token_type_ids = None,\n",
    "                        return_dict=False)\n",
    "\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = batch_labels.to('cpu').numpy()\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += self.calculate_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "            average_val_accuracy = val_accuracy / len(self.val_dataloader)\n",
    "            average_val_loss = val_loss / len(self.val_dataloader)\n",
    "            time_val = datetime.now() - t0_val\n",
    "\n",
    "            print(f'Average Loss:     {average_val_loss}')\n",
    "            print(f'Average Accuracy: {average_val_accuracy}')\n",
    "            print(f'Time Taken:       {time_val}\\n')\n",
    "\n",
    "            loss_dict['average training loss'].append(average_train_loss)\n",
    "            loss_dict['average validation loss'].append(average_val_loss)\n",
    "\n",
    "        print(f'Total training time: {datetime.now()-t0_train}')\n",
    "\n",
    "    def calculate_accuracy(self, preds, labels):\n",
    "        \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "        Parameters:\n",
    "            preds (np.array): The predicted label from the model\n",
    "            labels (np.array): The true label\n",
    "\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy as a percentage of the correct\n",
    "                predictions.\n",
    "        \"\"\"\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"Return the predicted probabilities of each class for input text.\n",
    "        \n",
    "        Parameters:\n",
    "            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n",
    "                the token IDs and attention masks for the text to perform\n",
    "                inference on.\n",
    "        \n",
    "        Returns:\n",
    "            probs (PyTorch.Tensor): A tensor containing the probability values\n",
    "                for each class as predicted by the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n",
    "                for t in batch)[:2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_token_ids, attention_mask=batch_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                #logits = self.model(batch_token_ids, batch_attention_mask)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "\n",
    "'''        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cde3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e556805",
   "metadata": {},
   "source": [
    "# Fine tune deberta-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f237a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652581ad",
   "metadata": {},
   "source": [
    "# Fine tune + Lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf978ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peftNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (1.9.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (0.33.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->peft) (0.21.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.16.0\n"
     ]
    }
   ],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef60ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ FineTuningPipeline (LoRA-ready)\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "\n",
    "\n",
    "class FineTuningPipeline:\n",
    "    def __init__(self, dataset, tokenizer, model, optimizer,\n",
    "                 loss_function=nn.CrossEntropyLoss(), val_size=0.1,\n",
    "                 epochs=4, seed=42):\n",
    "\n",
    "        self.df_dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.val_size = val_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.set_seeds()\n",
    "        self.token_ids, self.attention_masks = self.tokenize_dataset()\n",
    "        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        self.fine_tune()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "\n",
    "    def tokenize_dataset(self):\n",
    "        token_ids, attention_masks = [], []\n",
    "        for review in self.df_dataset['review_cleaned']:\n",
    "            ids, mask = self.tokenize(review)\n",
    "            token_ids.append(ids)\n",
    "            attention_masks.append(mask)\n",
    "        return torch.cat(token_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)\n",
    "        train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "            self.token_ids, self.attention_masks, labels, test_size=self.val_size, shuffle=False)\n",
    "\n",
    "        train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "        val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "        return DataLoader(train_data, shuffle=True, batch_size=32), DataLoader(val_data, batch_size=32)\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        total_steps = self.epochs * len(self.train_dataloader)\n",
    "        return get_linear_schedule_with_warmup(self.optimizer, 0, total_steps)\n",
    "\n",
    "    def set_seeds(self):\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        from datetime import datetime\n",
    "        print(f\"üîç Model type: {type(self.model)}\")\n",
    "        t0_train = datetime.now()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n===== Epoch {epoch+1}/{self.epochs} =====\")\n",
    "\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch in self.train_dataloader:\n",
    "                ids, mask, labels = [x.to(self.device) for x in batch]\n",
    "                self.model.zero_grad()\n",
    "                outputs = self.model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            print(f\"‚úÖ Avg Train Loss: {train_loss / len(self.train_dataloader):.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss, val_accuracy = 0, 0\n",
    "            t0_val = datetime.now()\n",
    "            for batch in self.val_dataloader:\n",
    "                ids, mask, labels = [x.to(self.device) for x in batch]\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += self.calculate_accuracy(logits.cpu().numpy(), labels.cpu().numpy())\n",
    "\n",
    "            val_time = datetime.now() - t0_val\n",
    "            print(f\"üß™ Avg Val Loss:  {val_loss / len(self.val_dataloader):.4f}\")\n",
    "            print(f\"üéØ Val Accuracy: {val_accuracy / len(self.val_dataloader):.4f}\")\n",
    "            print(f\"üïí Val Time:      {val_time}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Total training time: {datetime.now() - t0_train}\")\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        return np.sum(preds_flat == labels.flatten()) / len(labels)\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"Return the predicted probabilities of each class for input text.\n",
    "        \n",
    "        Parameters:\n",
    "            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n",
    "                the token IDs and attention masks for the text to perform\n",
    "                inference on.\n",
    "        \n",
    "        Returns:\n",
    "            probs (PyTorch.Tensor): A tensor containing the probability values\n",
    "                for each class as predicted by the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n",
    "                for t in batch)[:2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_token_ids, attention_mask=batch_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                #logits = self.model(batch_token_ids, batch_attention_mask)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ‚úÖ 1) ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ModernBERT (DeBERTa-v3)\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# ‚úÖ 2) ‡πÉ‡∏ä‡πâ dataset ‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß train ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "dataset = dataset.sample(15000, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60978deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Model type: <class 'peft.peft_model.PeftModelForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/3 =====\n",
      "‚úÖ Avg Train Loss: 0.3065\n",
      "üß™ Avg Val Loss:  0.2136\n",
      "üéØ Val Accuracy: 0.9130\n",
      "üïí Val Time:      0:00:23.439491\n",
      "\n",
      "===== Epoch 2/3 =====\n",
      "‚úÖ Avg Train Loss: 0.2144\n",
      "üß™ Avg Val Loss:  0.1979\n",
      "üéØ Val Accuracy: 0.9229\n",
      "üïí Val Time:      0:01:07.591575\n",
      "\n",
      "===== Epoch 3/3 =====\n",
      "‚úÖ Avg Train Loss: 0.1976\n",
      "üß™ Avg Val Loss:  0.2015\n",
      "üéØ Val Accuracy: 0.9233\n",
      "üïí Val Time:      0:00:32.793094\n",
      "\n",
      "‚úÖ Total training time: 1:08:21.390231\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# ‚úÖ 1) ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ModernBERT (DeBERTa-v3)\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# ‚úÖ 2) ‡πÉ‡∏ä‡πâ dataset ‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß train ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "dataset = dataset.sample(25000, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î base model\n",
    "\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\", num_labels=2\n",
    ")\n",
    "\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_proj\", \"value_proj\"],  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DeBERTa\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• LoRA\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=3,\n",
    "    seed=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b10102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_deberta_Lora_model\\\\tokenizer_config.json',\n",
       " 'merged_deberta_Lora_model\\\\special_tokens_map.json',\n",
       " 'merged_deberta_Lora_model\\\\spm.model',\n",
       " 'merged_deberta_Lora_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_deberta_Lora_model\")\n",
    "tokenizer.save_pretrained(\"merged_deberta_Lora_model\")\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f381a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_deberta_Lora_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_deberta_Lora_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_deberta_Lora_imdb\\\\spm.model',\n",
       " './fine_tuned_deberta_Lora_imdb\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Merge LoRA weights ‚Üí base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# ‚úÖ Save model + tokenizer\n",
    "save_path = \"./fine_tuned_deberta_Lora_imdb\"\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a256c1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/Lenovo/Desktop/NLP/Final_project/fine_tuned_deberta_Lora_imdb\\\\tokenizer_config.json',\n",
       " 'C:/Users/Lenovo/Desktop/NLP/Final_project/fine_tuned_deberta_Lora_imdb\\\\special_tokens_map.json',\n",
       " 'C:/Users/Lenovo/Desktop/NLP/Final_project/fine_tuned_deberta_Lora_imdb\\\\spm.model',\n",
       " 'C:/Users/Lenovo/Desktop/NLP/Final_project/fine_tuned_deberta_Lora_imdb\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Merge LoRA weights ‚Üí base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# ‚úÖ Save model + tokenizer\n",
    "save_path = 'C:/Users/Lenovo/Desktop/NLP/Final_project/fine_tuned_deberta_Lora_imdb'\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dba46cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_deberta_Lora_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_deberta_Lora_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_deberta_Lora_imdb\\\\spm.model',\n",
       " './fine_tuned_deberta_Lora_imdb\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Merge LoRA weights ‚Üí base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# ‚úÖ Save model + tokenizer\n",
    "save_path = './fine_tuned_deberta_Lora_imdb'\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 16:51:03.816008\n",
      "Average Loss:     0.287409131958092\n",
      "Time Taken:       1:00:32.755621\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 17:51:36.602769\n",
      "Average Loss:     0.20962147541502688\n",
      "Average Accuracy: 0.9207826747720365\n",
      "Time Taken:       0:00:08.150164\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 17:51:44.754942\n",
      "Average Loss:     0.16626481108049646\n",
      "Time Taken:       0:28:47.246521\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 18:20:32.004835\n",
      "Average Loss:     0.20655634182881802\n",
      "Average Accuracy: 0.9220174772036475\n",
      "Time Taken:       0:00:09.060500\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 18:20:41.067574\n",
      "Average Loss:     0.10255819945529951\n",
      "Time Taken:       0:36:45.293263\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 18:57:26.360837\n",
      "Average Loss:     0.2563236383919386\n",
      "Average Accuracy: 0.9220174772036475\n",
      "Time Taken:       0:00:11.521737\n",
      "\n",
      "Total training time: 2:06:34.068787\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ‚úÖ 1) ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ModernBERT (DeBERTa-v3)\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# ‚úÖ 2) ‡πÉ‡∏ä‡πâ dataset ‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß train ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "dataset = dataset.sample(15000, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ‚úÖ 3) ‡πÉ‡∏ä‡πâ learning rate ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Transformer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ‚úÖ 4) Fine-tune ‡πÅ‡∏Ñ‡πà 3 epoch + validation 10%\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65066f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏ö‡∏ô validation set\n",
    "predictions = fine_tuned_model.predict(fine_tuned_model.val_dataloader)\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á probability ‚Üí label 0/1\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74375a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9228\n"
     ]
    }
   ],
   "source": [
    "# 1) ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• predict ‡∏ö‡∏ô validation set\n",
    "predictions = fine_tuned_model.predict(fine_tuned_model.val_dataloader)\n",
    "\n",
    "# 2) ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô label 0/1\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 3) ‡∏î‡∏∂‡∏á label ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á validation set\n",
    "true_labels = fine_tuned_model.df_dataset['sentiment_encoded'][-len(predicted_labels):].values\n",
    "\n",
    "# 4) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy\n",
    "val_accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a08765",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_deberta_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_deberta_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_deberta_imdb\\\\spm.model',\n",
       " './fine_tuned_deberta_imdb\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "save_path = \"./fine_tuned_deberta_imdb\"\n",
    "\n",
    "# ‚úÖ save model weights + config\n",
    "fine_tuned_model.model.save_pretrained(save_path)\n",
    "\n",
    "# ‚úÖ save tokenizer ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô fine-tune\n",
    "fine_tuned_model.tokenizer.save_pretrained(save_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10859e1f",
   "metadata": {},
   "source": [
    "# Text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff69653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake\n",
      "Probabilities: [[0.97148424 0.02851575]]\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e069b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely movie using the look and devices of the noir cycle.\n",
      "Probabilities: [[0.00285796 0.997142  ]]\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely movie using the look and devices of the noir cycle.\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b0ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors. The story revolves around a ditzy cocktail waitress who becomes famous after inadvertently saving the life of an Arab dignitary. The story goes downhill halfway through the movie and Goldie's charm just doesn't save this movie. Unless you are a Goldie Hawn fan don't go out of your way to see this film.\n",
      "Probabilities: [[0.9945844  0.00541563]]\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors. The story revolves around a ditzy cocktail waitress who becomes famous after inadvertently saving the life of an Arab dignitary. The story goes downhill halfway through the movie and Goldie's charm just doesn't save this movie. Unless you are a Goldie Hawn fan don't go out of your way to see this film.\"\n",
    "\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b43a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Helen (Kate Capshaw) owns a bookstore in the sleepy, coastal town of Loblolly by the Sea. Divorced, Helen has a young daughter who is going to camp for the summer, giving mother a bit more freedom\n",
      "Probabilities: [[0.24761915 0.75238085]]\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors. The story revolves around a ditzy cocktail waitress who becomes famous after inadvertently saving the life of an Arab dignitary. The story goes downhill halfway through the movie and Goldie's charm just doesn't save this movie. Unless you are a Goldie Hawn fan don't go out of your way to see this film.\"\n",
    "\n",
    "text = \"Helen (Kate Capshaw) owns a bookstore in the sleepy, coastal town of Loblolly by the Sea. Divorced, Helen has a young daughter who is going to camp for the summer, giving mother a bit more freedom\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3c125",
   "metadata": {},
   "source": [
    "# Import model local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c883e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./fine_tuned_deberta_Lora_imdb\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c1ba5",
   "metadata": {},
   "source": [
    "# Predict Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68c86091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors\n",
      "Predicted: Negative ‚ùå\n"
     ]
    }
   ],
   "source": [
    "text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "import torch\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(\"Review:\" , text)\n",
    "print(\"Predicted:\", \"Positive ‚úÖ\" if probs.argmax() == 1 else \"Negative ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î CSV\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "# ‚úÖ tokenize ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "inputs = tokenizer(df[\"review\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# ‚úÖ predict\n",
    "outputs = model(**inputs)\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "pred_labels = probs.argmax(axis=1)\n",
    "\n",
    "# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "df[\"predicted_sentiment\"] = [\"positive\" if p == 1 else \"negative\" for p in pred_labels]\n",
    "\n",
    "# ‚úÖ ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "print(df.head())\n",
    "\n",
    "# ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•\n",
    "df.to_csv(\"reviews_with_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
