{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7742895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60731af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88030461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "df.head()\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf928eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the break tags (<br />)\n",
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "# Remove unnecessary whitespace\n",
    "df['review_cleaned'] = df['review_cleaned'].replace('\\s+', ' ', regex=True)\n",
    "\n",
    "# Compare 72 characters of the second review before and after cleaning\n",
    "print('Before cleaning:')\n",
    "print(df.iloc[1]['review'][0:72])\n",
    "\n",
    "print('\\nAfter cleaning:')\n",
    "print(df.iloc[1]['review_cleaned'][0:72])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36974862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].\\\n",
    "    apply(lambda x: 0 if x == 'negative' else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BertTokenizer(\n",
    "  name_or_path='bert-base-uncased',\n",
    "  vocab_size=30522,\n",
    "  model_max_length=512,\n",
    "  is_fast=False,\n",
    "  padding_side='right',\n",
    "  truncation_side='right',\n",
    "  special_tokens={\n",
    "    'unk_token': '[UNK]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'pad_token': '[PAD]',\n",
    "    'cls_token': '[CLS]',\n",
    "    'mask_token': '[MASK]'},\n",
    "  clean_up_tokenization_spaces=True),\n",
    "\n",
    "added_tokens_decoder={\n",
    "  0: AddedToken(\n",
    "    \"[PAD]\",\n",
    "    rstrip=False,\n",
    "    lstrip=False,\n",
    "    single_word=False,\n",
    "    normalized=False,  \n",
    "    special=True),\n",
    "\n",
    "  100: AddedToken(\n",
    "    \"[UNK]\",\n",
    "    rstrip=False,\n",
    "    lstrip=False,\n",
    "    single_word=False,\n",
    "    normalized=False,\n",
    "    special=True),\n",
    "\n",
    "  101: AddedToken(\n",
    "    \"[CLS]\",\n",
    "    rstrip=False,\n",
    "    lstrip=False,\n",
    "    single_word=False,\n",
    "    normalized=False,\n",
    "    special=True),\n",
    "\n",
    "  102: AddedToken(\n",
    "    \"[SEP]\",\n",
    "    rstrip=False,\n",
    "    lstrip=False,\n",
    "    single_word=False,\n",
    "    normalized=False,\n",
    "    special=True),\n",
    "\n",
    "  103: AddedToken(\n",
    "    \"[MASK]\",\n",
    "    rstrip=False,\n",
    "    lstrip=False,\n",
    "    single_word=False,\n",
    "    normalized=False,\n",
    "    special=True),\n",
    "  }\n",
    "\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a sample input sentence\n",
    "sample_sentence = 'I liked this movie'\n",
    "token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]\n",
    "print(f'Token IDs: {token_ids}')\n",
    "\n",
    "# Convert the token IDs back to tokens to reveal the special tokens added\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f'Tokens   : {tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "\n",
    "token_ids = tokenizer.encode(\n",
    "    review,\n",
    "    max_length = 512,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Encode each review\n",
    "for review in df['review_cleaned']:\n",
    "    batch_encoder = tokenizer.encode_plus(\n",
    "        review,\n",
    "        max_length = 512,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_tensors = 'pt')\n",
    "\n",
    "    token_ids.append(batch_encoder['input_ids'])\n",
    "    attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "# Convert token IDs and attention mask lists to PyTorch tensors\n",
    "token_ids = torch.cat(token_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "val_size = 0.1\n",
    "\n",
    "# Split the token IDs\n",
    "train_ids, val_ids = train_test_split(\n",
    "                        token_ids,\n",
    "                        test_size=val_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Split the attention masks\n",
    "train_masks, val_masks = train_test_split(\n",
    "                            attention_masks,\n",
    "                            test_size=val_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "# Split the labels\n",
    "labels = torch.tensor(df['sentiment_encoded'].values)\n",
    "train_labels, val_labels = train_test_split(\n",
    "                                labels,\n",
    "                                test_size=val_size,\n",
    "                                shuffle=False)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f83f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "# Loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available for faster training time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e05a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on:\", device)\n",
    "\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss, logits = model(\n",
    "            batch_token_ids,\n",
    "            token_type_ids = None,\n",
    "            attention_mask=batch_attention_mask,\n",
    "            labels=batch_labels,\n",
    "            return_dict=False)\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    average_train_loss = training_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "model.eval()\n",
    "val_loss = 0\n",
    "val_accuracy = 0\n",
    "\n",
    "for batch in val_dataloader:\n",
    "\n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            (loss, logits) = model(\n",
    "                batch_token_ids,\n",
    "                attention_mask = batch_attention_mask,\n",
    "                labels = batch_labels,\n",
    "                token_type_ids = None,\n",
    "                return_dict=False)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch_labels.to('cpu').numpy()\n",
    "        val_loss += loss.item()\n",
    "        val_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "average_val_accuracy = val_accuracy / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds, labels):\n",
    "    \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "    Parameters:\n",
    "        preds (np.array): The predicted label from the model\n",
    "        labels (np.array): The true label\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): The accuracy as a percentage of the correct\n",
    "            predictions.\n",
    "    \"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am very happy with my new phone!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "probs = torch.sigmoid(logits).cpu().numpy()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "predicted_labels = [labels[i] for i, v in enumerate(preds[0]) if v == 1]\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bf726",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am very happy with my new phone!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "probs = torch.sigmoid(logits).cpu().numpy()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# ถ้าใช้ labels เป็น list ของชื่อ label\n",
    "# predicted_labels = [labels[i] for i, v in enumerate(preds[0]) if v == 1]\n",
    "\n",
    "# หรือถ้าใช้ id2label dict\n",
    "predicted_labels = [id2label[i] for i, v in enumerate(preds[0]) if v == 1]\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94786ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['joy', 'optimism', 'love', 'anger', 'sadness', 'fear']  # หรือ label ของคุณเอง\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "text = \"I am very happy with my new phone!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "probs = torch.sigmoid(logits).cpu().numpy()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "predicted_labels = [labels[i] for i, v in enumerate(preds[0]) if v == 1]\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
