{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed986337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_247528\\586864222.py:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  replace('\\s+', ' ', regex=True)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(path):\n",
    "    \"\"\" Remove unnecessary characters and encode the sentiment labels.\n",
    "\n",
    "    The type of preprocessing required changes based on the dataset. For the\n",
    "    IMDb dataset, the review texts contains HTML break tags (<br/>) leftover\n",
    "    from the scraping process, and some unnecessary whitespace, which are\n",
    "    removed. Finally, encode the sentiment labels as 0 for \"negative\" and 1 for\n",
    "    \"positive\". This method assumes the dataset file contains the headers\n",
    "    \"review\" and \"sentiment\".\n",
    "\n",
    "    Parameters:\n",
    "        path (str): A path to a dataset file containing the sentiment analysis\n",
    "            dataset. The structure of the file should be as follows: one column\n",
    "            called \"review\" containing the review text, and one column called\n",
    "            \"sentiment\" containing the ground truth label. The label options\n",
    "            should be \"negative\" and \"positive\".\n",
    "\n",
    "    Returns:\n",
    "        df_dataset (pd.DataFrame): A DataFrame containing the raw data\n",
    "            loaded from the self.dataset path. In addition to the expected\n",
    "            \"review\" and \"sentiment\" columns, are:\n",
    "\n",
    "            > review_cleaned - a copy of the \"review\" column with the HTML\n",
    "                break tags and unnecessary whitespace removed\n",
    "\n",
    "            > sentiment_encoded - a copy of the \"sentiment\" column with the\n",
    "                \"negative\" values mapped to 0 and \"positive\" values mapped\n",
    "                to 1\n",
    "    \"\"\"\n",
    "    df_dataset = pd.read_csv(path)\n",
    "\n",
    "    df_dataset['review_cleaned'] = df_dataset['review'].\\\n",
    "        apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "    df_dataset['review_cleaned'] = df_dataset['review_cleaned'].\\\n",
    "        replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    df_dataset['sentiment_encoded'] = df_dataset['sentiment'].\\\n",
    "        apply(lambda x: 0 if x == 'negative' else 1)\n",
    "\n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "5  Probably my all-time favorite movie, a story o...  positive   \n",
      "6  I sure would like to see a resurrection of a u...  positive   \n",
      "7  This show was an amazing, fresh & innovative i...  negative   \n",
      "8  Encouraged by the positive comments about this...  negative   \n",
      "9  If you like original gut wrenching laughter yo...  positive   \n",
      "\n",
      "                                      review_cleaned  sentiment_encoded  \n",
      "0  One of the other reviewers has mentioned that ...                  1  \n",
      "1  A wonderful little production. The filming tec...                  1  \n",
      "2  I thought this was a wonderful way to spend ti...                  1  \n",
      "3  Basically there's a family where a little boy ...                  0  \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...                  1  \n",
      "5  Probably my all-time favorite movie, a story o...                  1  \n",
      "6  I sure would like to see a resurrection of a u...                  1  \n",
      "7  This show was an amazing, fresh & innovative i...                  0  \n",
      "8  Encouraged by the positive comments about this...                  0  \n",
      "9  If you like original gut wrenching laughter yo...                  1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = preprocess_dataset(\"C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv\")\n",
    "\n",
    "print(dataset.head(10))  # ดูตัวอย่าง 5 แถวแรก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd22781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "\n",
    "\n",
    "class FineTuningPipeline:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function = nn.CrossEntropyLoss(),\n",
    "            val_size = 0.1,\n",
    "            epochs = 4,\n",
    "            seed = 42):\n",
    "\n",
    "        self.df_dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.val_size = val_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "        # Check if GPU is available for faster training time\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        # Perform fine-tuning\n",
    "        self.model.to(self.device)\n",
    "        self.set_seeds()\n",
    "        self.token_ids, self.attention_masks = self.tokenize_dataset()\n",
    "        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        self.fine_tune()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Tokenize input text and return the token IDs and attention mask.\n",
    "\n",
    "        Tokenize an input string, setting a maximum length of 512 tokens.\n",
    "        Sequences with more than 512 tokens will be truncated to this limit,\n",
    "        and sequences with less than 512 tokens will be supplemented with [PAD]\n",
    "        tokens to bring them up to this limit. The datatype of the returned\n",
    "        tensors will be the PyTorch tensor format. These return values are\n",
    "        tensors of size 1 x max_length where max_length is the maximum number\n",
    "        of tokens per input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs for each token in\n",
    "                the input sequence.\n",
    "\n",
    "            attention_mask (torch.Tensor): A tensor of 1s and 0s where a 1\n",
    "                indicates a token can be attended to during the attention\n",
    "                process, and a 0 indicates a token should be ignored. This is\n",
    "                used to prevent BERT from attending to [PAD] tokens during its\n",
    "                training/inference.\n",
    "        \"\"\"\n",
    "        batch_encoder = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length = 128,\n",
    "            #max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt')\n",
    "\n",
    "        token_ids = batch_encoder['input_ids']\n",
    "        attention_mask = batch_encoder['attention_mask']\n",
    "\n",
    "        return token_ids, attention_mask\n",
    "\n",
    "    def tokenize_dataset(self):\n",
    "        \"\"\" Apply the self.tokenize method to the fine-tuning dataset.\n",
    "\n",
    "        Tokenize and return the input sequence for each row in the fine-tuning\n",
    "        dataset given by self.dataset. The return values are tensors of size\n",
    "        len_dataset x max_length where len_dataset is the number of rows in the\n",
    "        fine-tuning dataset and max_length is the maximum number of tokens per\n",
    "        input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of tensors containing token IDs\n",
    "            for each token in the input sequence.\n",
    "\n",
    "            attention_masks (torch.Tensor): A tensor of tensors containing the\n",
    "                attention masks for each sequence in the fine-tuning dataset.\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for review in self.df_dataset['review_cleaned']:\n",
    "            tokens, masks = self.tokenize(review)\n",
    "            token_ids.append(tokens)\n",
    "            attention_masks.append(masks)\n",
    "\n",
    "        token_ids = torch.cat(token_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return token_ids, attention_masks\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        \"\"\" Create dataloaders for the train and validation set.\n",
    "\n",
    "        Split the tokenized dataset into train and validation sets according to\n",
    "        the self.val_size value. For example, if self.val_size is set to 0.1,\n",
    "        90% of the data will be used to form the train set, and 10% for the\n",
    "        validation set. Convert the \"sentiment_encoded\" column (labels for each\n",
    "        row) to PyTorch tensors to be used in the dataloaders.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            train_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the train data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "            val_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the validation data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "        \"\"\"\n",
    "        train_ids, val_ids = train_test_split(\n",
    "                        self.token_ids,\n",
    "                        test_size=self.val_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "        train_masks, val_masks = train_test_split(\n",
    "                                    self.attention_masks,\n",
    "                                    test_size=self.val_size,\n",
    "                                    shuffle=False)\n",
    "\n",
    "        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)\n",
    "        train_labels, val_labels = train_test_split(\n",
    "                                        labels,\n",
    "                                        test_size=self.val_size,\n",
    "                                        shuffle=False)\n",
    "\n",
    "        train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=32) # batch_size = 16 before\n",
    "        val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=32) # batch_size = 16 before\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        \"\"\" Create a linear scheduler for the learning rate.\n",
    "\n",
    "        Create a scheduler with a learning rate that increases linearly from 0\n",
    "        to a maximum value (called the warmup period), then decreases linearly\n",
    "        to 0 again. num_warmup_steps is set to 0 here based on an example from\n",
    "        Hugging Face:\n",
    "\n",
    "        https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2\n",
    "        d008813037968a9e58/examples/run_glue.py#L308\n",
    "\n",
    "        Read more about schedulers here:\n",
    "\n",
    "        https://huggingface.co/docs/transformers/main_classes/optimizer_\n",
    "        schedules#transformers.get_linear_schedule_with_warmup\n",
    "        \"\"\"\n",
    "        num_training_steps = self.epochs * len(self.train_dataloader)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps)\n",
    "\n",
    "        return scheduler\n",
    "\n",
    "    def set_seeds(self):\n",
    "        \"\"\" Set the random seeds so that results are reproduceable.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        \"\"\"Train the classification head on the BERT model.\n",
    "\n",
    "        Fine-tune the model by training the classification head (linear layer)\n",
    "        sitting on top of the BERT model. The model trained on the data in the\n",
    "        self.train_dataloader, and validated at the end of each epoch on the\n",
    "        data in the self.val_dataloader. The series of steps are described\n",
    "        below:\n",
    "\n",
    "        Training:\n",
    "\n",
    "        > Create a dictionary to store the average training loss and average\n",
    "          validation loss for each epoch.\n",
    "        > Store the time at the start of training, this is used to calculate\n",
    "          the time taken for the entire training process.\n",
    "        > Begin a loop to train the model for each epoch in self.epochs.\n",
    "\n",
    "        For each epoch:\n",
    "\n",
    "        > Switch the model to train mode. This will cause the model to behave\n",
    "          differently than when in evaluation mode (e.g. the batchnorm and\n",
    "          dropout layers are activated in train mode, but disabled in\n",
    "          evaluation mode).\n",
    "        > Set the training loss to 0 for the start of the epoch. This is used\n",
    "          to track the loss of the model on the training data over subsequent\n",
    "          epochs. The loss should decrease with each epoch if training is\n",
    "          successful.\n",
    "        > Store the time at the start of the epoch, this is used to calculate\n",
    "          the time taken for the epoch to be completed.\n",
    "        > As per the BERT authors' recommendations, the training data for each\n",
    "          epoch is split into batches. Loop through the training process for\n",
    "          each batch.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the zero_grad method to reset the calculated gradients from\n",
    "          the previous iteration of this loop.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Increment the total loss for the epoch. The loss is returned from the\n",
    "          model as a PyTorch tensor so extract the float value using the item\n",
    "          method.\n",
    "        > Perform a backward pass of the model and propagate the loss through\n",
    "          the classifier head. This will allow the model to determine what\n",
    "          adjustments to make to the weights and biases to improve its\n",
    "          performance on the batch.\n",
    "        > Clip the gradients to be no larger than 1.0 so the model does not\n",
    "          suffer from the exploding gradients problem.\n",
    "        > Call the optimizer to take a step in the direction of the error\n",
    "          surface as determined by the backward pass.\n",
    "\n",
    "        After training on each batch:\n",
    "\n",
    "        > Calculate the average loss and time taken for training on the epoch.\n",
    "\n",
    "        Validation step for the epoch:\n",
    "\n",
    "        > Switch the model to evaluation mode.\n",
    "        > Set the validation loss to 0. This is used to track the loss of the\n",
    "          model on the validation data over subsequent epochs. The loss should\n",
    "          decrease with each epoch if training was successful.\n",
    "        > Store the time at the start of the validation, this is used to\n",
    "          calculate the time taken for the validation for this epoch to be\n",
    "          completed.\n",
    "        > Split the validation data into batches.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the no_grad method to instruct the model not to calculate the\n",
    "          gradients since we wil not be performing any optimization steps here,\n",
    "          only inference.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Extract the logits and labels from the model and move them to the CPU\n",
    "          (if they are not already there).\n",
    "        > Increment the loss and calculate the accuracy based on the true\n",
    "          labels in the validation dataloader.\n",
    "        > Calculate the average loss and accuracy, and add these to the loss\n",
    "          dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = {\n",
    "            'epoch': [i+1 for i in range(self.epochs)],\n",
    "            'average training loss': [],\n",
    "            'average validation loss': []\n",
    "        }\n",
    "\n",
    "        t0_train = datetime.now()\n",
    "\n",
    "        for epoch in range(0, self.epochs):\n",
    "\n",
    "            # Train step\n",
    "            self.model.train()\n",
    "            training_loss = 0\n",
    "            t0_epoch = datetime.now()\n",
    "\n",
    "            print(f'{\"-\"*20} Epoch {epoch+1} {\"-\"*20}')\n",
    "            print('\\nTraining:\\n---------')\n",
    "            print(f'Start Time:       {t0_epoch}')\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                loss, logits = self.model(\n",
    "                    batch_token_ids,\n",
    "                    token_type_ids = None,\n",
    "                    attention_mask=batch_attention_mask,\n",
    "                    labels=batch_labels,\n",
    "                    return_dict=False)\n",
    "\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            average_train_loss = training_loss / len(self.train_dataloader)\n",
    "            time_epoch = datetime.now() - t0_epoch\n",
    "\n",
    "            print(f'Average Loss:     {average_train_loss}')\n",
    "            print(f'Time Taken:       {time_epoch}')\n",
    "\n",
    "            # Validation step\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            t0_val = datetime.now()\n",
    "\n",
    "            print('\\nValidation:\\n---------')\n",
    "            print(f'Start Time:       {t0_val}')\n",
    "\n",
    "            for batch in self.val_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    (loss, logits) = self.model(\n",
    "                        batch_token_ids,\n",
    "                        attention_mask = batch_attention_mask,\n",
    "                        labels = batch_labels,\n",
    "                        token_type_ids = None,\n",
    "                        return_dict=False)\n",
    "\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = batch_labels.to('cpu').numpy()\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += self.calculate_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "            average_val_accuracy = val_accuracy / len(self.val_dataloader)\n",
    "            average_val_loss = val_loss / len(self.val_dataloader)\n",
    "            time_val = datetime.now() - t0_val\n",
    "\n",
    "            print(f'Average Loss:     {average_val_loss}')\n",
    "            print(f'Average Accuracy: {average_val_accuracy}')\n",
    "            print(f'Time Taken:       {time_val}\\n')\n",
    "\n",
    "            loss_dict['average training loss'].append(average_train_loss)\n",
    "            loss_dict['average validation loss'].append(average_val_loss)\n",
    "\n",
    "        print(f'Total training time: {datetime.now()-t0_train}')\n",
    "\n",
    "    def calculate_accuracy(self, preds, labels):\n",
    "        \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "        Parameters:\n",
    "            preds (np.array): The predicted label from the model\n",
    "            labels (np.array): The true label\n",
    "\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy as a percentage of the correct\n",
    "                predictions.\n",
    "        \"\"\"\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"Return the predicted probabilities of each class for input text.\n",
    "        \n",
    "        Parameters:\n",
    "            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n",
    "                the token IDs and attention masks for the text to perform\n",
    "                inference on.\n",
    "        \n",
    "        Returns:\n",
    "            probs (PyTorch.Tensor): A tensor containing the probability values\n",
    "                for each class as predicted by the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n",
    "                for t in batch)[:2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_token_ids, attention_mask=batch_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                #logits = self.model(batch_token_ids, batch_attention_mask)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5edd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 20:49:41.385674\n",
      "Average Loss:     0.2849893314006274\n",
      "Time Taken:       0:06:51.487562\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 20:56:32.879866\n",
      "Average Loss:     0.22452778179967214\n",
      "Average Accuracy: 0.908234126984127\n",
      "Time Taken:       0:00:12.987049\n",
      "\n",
      "-------------------- Epoch 2 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 20:56:45.866915\n",
      "Average Loss:     0.13641506602299494\n",
      "Time Taken:       0:06:36.465234\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:03:22.335559\n",
      "Average Loss:     0.2682568696401422\n",
      "Average Accuracy: 0.9072420634920635\n",
      "Time Taken:       0:00:13.158859\n",
      "\n",
      "-------------------- Epoch 3 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:03:35.502880\n",
      "Average Loss:     0.042031059106030264\n",
      "Time Taken:       0:06:37.321795\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:10:12.824675\n",
      "Average Loss:     0.43149672470809447\n",
      "Average Accuracy: 0.9032738095238095\n",
      "Time Taken:       0:00:13.212948\n",
      "\n",
      "-------------------- Epoch 4 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:10:26.037623\n",
      "Average Loss:     0.009903954374824565\n",
      "Time Taken:       0:06:37.410467\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:17:03.456529\n",
      "Average Loss:     0.8858761727641404\n",
      "Average Accuracy: 0.9047619047619048\n",
      "Time Taken:       0:00:13.185200\n",
      "\n",
      "-------------------- Epoch 5 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:17:16.641729\n",
      "Average Loss:     0.0031051541935447156\n",
      "Time Taken:       0:05:25.637545\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:22:42.282276\n",
      "Average Loss:     1.1266692656837838\n",
      "Average Accuracy: 0.9017857142857143\n",
      "Time Taken:       0:00:10.421332\n",
      "\n",
      "-------------------- Epoch 6 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:22:52.706009\n",
      "Average Loss:     0.000850578790661914\n",
      "Time Taken:       0:05:13.131989\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:28:05.839995\n",
      "Average Loss:     1.176730813435406\n",
      "Average Accuracy: 0.8998015873015873\n",
      "Time Taken:       0:00:10.414170\n",
      "\n",
      "-------------------- Epoch 7 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:28:16.256673\n",
      "Average Loss:     0.000204980329075907\n",
      "Time Taken:       0:05:14.162589\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:33:30.421449\n",
      "Average Loss:     1.2000408473062594\n",
      "Average Accuracy: 0.90625\n",
      "Time Taken:       0:00:10.386434\n",
      "\n",
      "-------------------- Epoch 8 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:33:40.810401\n",
      "Average Loss:     2.7577041588884758e-05\n",
      "Time Taken:       0:05:14.633047\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:38:55.445447\n",
      "Average Loss:     1.2563797352621053\n",
      "Average Accuracy: 0.9057539682539683\n",
      "Time Taken:       0:00:10.428609\n",
      "\n",
      "-------------------- Epoch 9 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:39:05.876053\n",
      "Average Loss:     0.0001812018291027593\n",
      "Time Taken:       0:05:13.987591\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:44:19.865645\n",
      "Average Loss:     1.1778969142840665\n",
      "Average Accuracy: 0.9032738095238095\n",
      "Time Taken:       0:00:10.424102\n",
      "\n",
      "-------------------- Epoch 10 --------------------\n",
      "\n",
      "Training:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:44:30.292746\n",
      "Average Loss:     6.453836246828453e-06\n",
      "Time Taken:       0:05:14.568161\n",
      "\n",
      "Validation:\n",
      "---------\n",
      "Start Time:       2025-07-24 21:49:44.862910\n",
      "Average Loss:     1.193738047940403\n",
      "Average Accuracy: 0.9032738095238095\n",
      "Time Taken:       0:00:10.400759\n",
      "\n",
      "Total training time: 1:00:13.878997\n"
     ]
    }
   ],
   "source": [
    "# Initialise parameters\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "\n",
    "dataset = dataset.sample(20000, random_state=42)\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Fine-tune model using class\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    val_size = 0.1,\n",
    "    epochs = 10,\n",
    "    seed = 42\n",
    "    )\n",
    "\n",
    "# Make some predictions using the validation dataset\n",
    "\n",
    "\n",
    "#model.predict(model.val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65066f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# ทำนายผลบน validation set\n",
    "predictions = fine_tuned_model.predict(fine_tuned_model.val_dataloader)\n",
    "\n",
    "# แปลง probability → label 0/1\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74375a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9030\n"
     ]
    }
   ],
   "source": [
    "# 1) ให้โมเดล predict บน validation set\n",
    "predictions = fine_tuned_model.predict(fine_tuned_model.val_dataloader)\n",
    "\n",
    "# 2) แปลงเป็น label 0/1\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 3) ดึง label จริงของ validation set\n",
    "true_labels = fine_tuned_model.df_dataset['sentiment_encoded'][-len(predicted_labels):].values\n",
    "\n",
    "# 4) คำนวณ accuracy\n",
    "val_accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15fde9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_modernBERT_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_modernBERT_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_modernBERT_imdb\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"./fine_tuned_modernBERT_imdb\"\n",
    "\n",
    "# ✅ save model weights + config\n",
    "fine_tuned_model.model.save_pretrained(save_path)\n",
    "\n",
    "# ✅ save tokenizer ที่ใช้ตอน fine-tune\n",
    "fine_tuned_model.tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c653e4",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd98959",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./fine_tuned_modernBERT_imdb\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb1f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors\n",
      "Predicted: Negative ❌\n"
     ]
    }
   ],
   "source": [
    "text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "import torch\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(\"Review:\" , text)\n",
    "print(\"Predicted:\", \"Positive ✅\" if probs.argmax() == 1 else \"Negative ❌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff69653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake\n",
      "Probabilities: [[1.0000000e+00 2.4967246e-08]]\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# สร้าง DataLoader สำหรับข้อความเดียว\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ใช้ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e069b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely movie using the look and devices of the noir cycle.\n",
      "Probabilities: [[3.892706e-05 9.999610e-01]]\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely movie using the look and devices of the noir cycle.\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# สร้าง DataLoader สำหรับข้อความเดียว\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ใช้ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e9f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
