{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed986337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_55448\\586864222.py:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  replace('\\s+', ' ', regex=True)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(path):\n",
    "    \"\"\" Remove unnecessary characters and encode the sentiment labels.\n",
    "\n",
    "    The type of preprocessing required changes based on the dataset. For the\n",
    "    IMDb dataset, the review texts contains HTML break tags (<br/>) leftover\n",
    "    from the scraping process, and some unnecessary whitespace, which are\n",
    "    removed. Finally, encode the sentiment labels as 0 for \"negative\" and 1 for\n",
    "    \"positive\". This method assumes the dataset file contains the headers\n",
    "    \"review\" and \"sentiment\".\n",
    "\n",
    "    Parameters:\n",
    "        path (str): A path to a dataset file containing the sentiment analysis\n",
    "            dataset. The structure of the file should be as follows: one column\n",
    "            called \"review\" containing the review text, and one column called\n",
    "            \"sentiment\" containing the ground truth label. The label options\n",
    "            should be \"negative\" and \"positive\".\n",
    "\n",
    "    Returns:\n",
    "        df_dataset (pd.DataFrame): A DataFrame containing the raw data\n",
    "            loaded from the self.dataset path. In addition to the expected\n",
    "            \"review\" and \"sentiment\" columns, are:\n",
    "\n",
    "            > review_cleaned - a copy of the \"review\" column with the HTML\n",
    "                break tags and unnecessary whitespace removed\n",
    "\n",
    "            > sentiment_encoded - a copy of the \"sentiment\" column with the\n",
    "                \"negative\" values mapped to 0 and \"positive\" values mapped\n",
    "                to 1\n",
    "    \"\"\"\n",
    "    df_dataset = pd.read_csv(path)\n",
    "\n",
    "    df_dataset['review_cleaned'] = df_dataset['review'].\\\n",
    "        apply(lambda x: x.replace('<br />', ''))\n",
    "\n",
    "    df_dataset['review_cleaned'] = df_dataset['review_cleaned'].\\\n",
    "        replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    df_dataset['sentiment_encoded'] = df_dataset['sentiment'].\\\n",
    "        apply(lambda x: 0 if x == 'negative' else 1)\n",
    "\n",
    "    return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "5  Probably my all-time favorite movie, a story o...  positive   \n",
      "6  I sure would like to see a resurrection of a u...  positive   \n",
      "7  This show was an amazing, fresh & innovative i...  negative   \n",
      "8  Encouraged by the positive comments about this...  negative   \n",
      "9  If you like original gut wrenching laughter yo...  positive   \n",
      "\n",
      "                                      review_cleaned  sentiment_encoded  \n",
      "0  One of the other reviewers has mentioned that ...                  1  \n",
      "1  A wonderful little production. The filming tec...                  1  \n",
      "2  I thought this was a wonderful way to spend ti...                  1  \n",
      "3  Basically there's a family where a little boy ...                  0  \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...                  1  \n",
      "5  Probably my all-time favorite movie, a story o...                  1  \n",
      "6  I sure would like to see a resurrection of a u...                  1  \n",
      "7  This show was an amazing, fresh & innovative i...                  0  \n",
      "8  Encouraged by the positive comments about this...                  0  \n",
      "9  If you like original gut wrenching laughter yo...                  1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = preprocess_dataset(\"C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv\")\n",
    "\n",
    "print(dataset.head(10))  # ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "\n",
    "\n",
    "class FineTuningPipeline:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_function = nn.CrossEntropyLoss(),\n",
    "            val_size = 0.1,\n",
    "            epochs = 4,\n",
    "            seed = 42):\n",
    "\n",
    "        self.df_dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.val_size = val_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "        # Check if GPU is available for faster training time\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda:0')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        # Perform fine-tuning\n",
    "        self.model.to(self.device)\n",
    "        self.set_seeds()\n",
    "        self.token_ids, self.attention_masks = self.tokenize_dataset()\n",
    "        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        self.fine_tune()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Tokenize input text and return the token IDs and attention mask.\n",
    "\n",
    "        Tokenize an input string, setting a maximum length of 512 tokens.\n",
    "        Sequences with more than 512 tokens will be truncated to this limit,\n",
    "        and sequences with less than 512 tokens will be supplemented with [PAD]\n",
    "        tokens to bring them up to this limit. The datatype of the returned\n",
    "        tensors will be the PyTorch tensor format. These return values are\n",
    "        tensors of size 1 x max_length where max_length is the maximum number\n",
    "        of tokens per input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of token IDs for each token in\n",
    "                the input sequence.\n",
    "\n",
    "            attention_mask (torch.Tensor): A tensor of 1s and 0s where a 1\n",
    "                indicates a token can be attended to during the attention\n",
    "                process, and a 0 indicates a token should be ignored. This is\n",
    "                used to prevent BERT from attending to [PAD] tokens during its\n",
    "                training/inference.\n",
    "        \"\"\"\n",
    "        batch_encoder = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length = 128,\n",
    "            #max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt')\n",
    "\n",
    "        token_ids = batch_encoder['input_ids']\n",
    "        attention_mask = batch_encoder['attention_mask']\n",
    "\n",
    "        return token_ids, attention_mask\n",
    "\n",
    "    def tokenize_dataset(self):\n",
    "        \"\"\" Apply the self.tokenize method to the fine-tuning dataset.\n",
    "\n",
    "        Tokenize and return the input sequence for each row in the fine-tuning\n",
    "        dataset given by self.dataset. The return values are tensors of size\n",
    "        len_dataset x max_length where len_dataset is the number of rows in the\n",
    "        fine-tuning dataset and max_length is the maximum number of tokens per\n",
    "        input sequence (512 for BERT).\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            token_ids (torch.Tensor): A tensor of tensors containing token IDs\n",
    "            for each token in the input sequence.\n",
    "\n",
    "            attention_masks (torch.Tensor): A tensor of tensors containing the\n",
    "                attention masks for each sequence in the fine-tuning dataset.\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for review in self.df_dataset['review_cleaned']:\n",
    "            tokens, masks = self.tokenize(review)\n",
    "            token_ids.append(tokens)\n",
    "            attention_masks.append(masks)\n",
    "\n",
    "        token_ids = torch.cat(token_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return token_ids, attention_masks\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        \"\"\" Create dataloaders for the train and validation set.\n",
    "\n",
    "        Split the tokenized dataset into train and validation sets according to\n",
    "        the self.val_size value. For example, if self.val_size is set to 0.1,\n",
    "        90% of the data will be used to form the train set, and 10% for the\n",
    "        validation set. Convert the \"sentiment_encoded\" column (labels for each\n",
    "        row) to PyTorch tensors to be used in the dataloaders.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            train_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the train data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "            val_dataloader (torch.utils.data.dataloader.DataLoader): A\n",
    "                dataloader of the validation data, including the token IDs,\n",
    "                attention masks, and sentiment labels.\n",
    "\n",
    "        \"\"\"\n",
    "        train_ids, val_ids = train_test_split(\n",
    "                        self.token_ids,\n",
    "                        test_size=self.val_size,\n",
    "                        shuffle=False)\n",
    "\n",
    "        train_masks, val_masks = train_test_split(\n",
    "                                    self.attention_masks,\n",
    "                                    test_size=self.val_size,\n",
    "                                    shuffle=False)\n",
    "\n",
    "        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)\n",
    "        train_labels, val_labels = train_test_split(\n",
    "                                        labels,\n",
    "                                        test_size=self.val_size,\n",
    "                                        shuffle=False)\n",
    "\n",
    "        train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=32) # batch_size = 16 before\n",
    "        val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=32) # batch_size = 16 before\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        \"\"\" Create a linear scheduler for the learning rate.\n",
    "\n",
    "        Create a scheduler with a learning rate that increases linearly from 0\n",
    "        to a maximum value (called the warmup period), then decreases linearly\n",
    "        to 0 again. num_warmup_steps is set to 0 here based on an example from\n",
    "        Hugging Face:\n",
    "\n",
    "        https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2\n",
    "        d008813037968a9e58/examples/run_glue.py#L308\n",
    "\n",
    "        Read more about schedulers here:\n",
    "\n",
    "        https://huggingface.co/docs/transformers/main_classes/optimizer_\n",
    "        schedules#transformers.get_linear_schedule_with_warmup\n",
    "        \"\"\"\n",
    "        num_training_steps = self.epochs * len(self.train_dataloader)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps)\n",
    "\n",
    "        return scheduler\n",
    "\n",
    "    def set_seeds(self):\n",
    "        \"\"\" Set the random seeds so that results are reproduceable.\n",
    "\n",
    "        Parameters:\n",
    "            None.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        \"\"\"Train the classification head on the BERT model.\n",
    "\n",
    "        Fine-tune the model by training the classification head (linear layer)\n",
    "        sitting on top of the BERT model. The model trained on the data in the\n",
    "        self.train_dataloader, and validated at the end of each epoch on the\n",
    "        data in the self.val_dataloader. The series of steps are described\n",
    "        below:\n",
    "\n",
    "        Training:\n",
    "\n",
    "        > Create a dictionary to store the average training loss and average\n",
    "          validation loss for each epoch.\n",
    "        > Store the time at the start of training, this is used to calculate\n",
    "          the time taken for the entire training process.\n",
    "        > Begin a loop to train the model for each epoch in self.epochs.\n",
    "\n",
    "        For each epoch:\n",
    "\n",
    "        > Switch the model to train mode. This will cause the model to behave\n",
    "          differently than when in evaluation mode (e.g. the batchnorm and\n",
    "          dropout layers are activated in train mode, but disabled in\n",
    "          evaluation mode).\n",
    "        > Set the training loss to 0 for the start of the epoch. This is used\n",
    "          to track the loss of the model on the training data over subsequent\n",
    "          epochs. The loss should decrease with each epoch if training is\n",
    "          successful.\n",
    "        > Store the time at the start of the epoch, this is used to calculate\n",
    "          the time taken for the epoch to be completed.\n",
    "        > As per the BERT authors' recommendations, the training data for each\n",
    "          epoch is split into batches. Loop through the training process for\n",
    "          each batch.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the zero_grad method to reset the calculated gradients from\n",
    "          the previous iteration of this loop.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Increment the total loss for the epoch. The loss is returned from the\n",
    "          model as a PyTorch tensor so extract the float value using the item\n",
    "          method.\n",
    "        > Perform a backward pass of the model and propagate the loss through\n",
    "          the classifier head. This will allow the model to determine what\n",
    "          adjustments to make to the weights and biases to improve its\n",
    "          performance on the batch.\n",
    "        > Clip the gradients to be no larger than 1.0 so the model does not\n",
    "          suffer from the exploding gradients problem.\n",
    "        > Call the optimizer to take a step in the direction of the error\n",
    "          surface as determined by the backward pass.\n",
    "\n",
    "        After training on each batch:\n",
    "\n",
    "        > Calculate the average loss and time taken for training on the epoch.\n",
    "\n",
    "        Validation step for the epoch:\n",
    "\n",
    "        > Switch the model to evaluation mode.\n",
    "        > Set the validation loss to 0. This is used to track the loss of the\n",
    "          model on the validation data over subsequent epochs. The loss should\n",
    "          decrease with each epoch if training was successful.\n",
    "        > Store the time at the start of the validation, this is used to\n",
    "          calculate the time taken for the validation for this epoch to be\n",
    "          completed.\n",
    "        > Split the validation data into batches.\n",
    "\n",
    "        For each batch:\n",
    "\n",
    "        > Move the token IDs, attention masks, and labels to the GPU if\n",
    "          available for faster processing, otherwise these will be kept on the\n",
    "          CPU.\n",
    "        > Invoke the no_grad method to instruct the model not to calculate the\n",
    "          gradients since we wil not be performing any optimization steps here,\n",
    "          only inference.\n",
    "        > Pass the batch to the model to calculate the logits (predictions\n",
    "          based on the current classifier weights and biases) as well as the\n",
    "          loss.\n",
    "        > Extract the logits and labels from the model and move them to the CPU\n",
    "          (if they are not already there).\n",
    "        > Increment the loss and calculate the accuracy based on the true\n",
    "          labels in the validation dataloader.\n",
    "        > Calculate the average loss and accuracy, and add these to the loss\n",
    "          dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = {\n",
    "            'epoch': [i+1 for i in range(self.epochs)],\n",
    "            'average training loss': [],\n",
    "            'average validation loss': []\n",
    "        }\n",
    "\n",
    "        t0_train = datetime.now()\n",
    "\n",
    "        for epoch in range(0, self.epochs):\n",
    "\n",
    "            # Train step\n",
    "            self.model.train()\n",
    "            training_loss = 0\n",
    "            t0_epoch = datetime.now()\n",
    "\n",
    "            print(f'{\"-\"*20} Epoch {epoch+1} {\"-\"*20}')\n",
    "            print('\\nTraining:\\n---------')\n",
    "            print(f'Start Time:       {t0_epoch}')\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                loss, logits = self.model(\n",
    "                    batch_token_ids,\n",
    "                    token_type_ids = None,\n",
    "                    attention_mask=batch_attention_mask,\n",
    "                    labels=batch_labels,\n",
    "                    return_dict=False)\n",
    "\n",
    "                training_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            average_train_loss = training_loss / len(self.train_dataloader)\n",
    "            time_epoch = datetime.now() - t0_epoch\n",
    "\n",
    "            print(f'Average Loss:     {average_train_loss}')\n",
    "            print(f'Time Taken:       {time_epoch}')\n",
    "\n",
    "            # Validation step\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            t0_val = datetime.now()\n",
    "\n",
    "            print('\\nValidation:\\n---------')\n",
    "            print(f'Start Time:       {t0_val}')\n",
    "\n",
    "            for batch in self.val_dataloader:\n",
    "\n",
    "                batch_token_ids = batch[0].to(self.device)\n",
    "                batch_attention_mask = batch[1].to(self.device)\n",
    "                batch_labels = batch[2].to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    (loss, logits) = self.model(\n",
    "                        batch_token_ids,\n",
    "                        attention_mask = batch_attention_mask,\n",
    "                        labels = batch_labels,\n",
    "                        token_type_ids = None,\n",
    "                        return_dict=False)\n",
    "\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = batch_labels.to('cpu').numpy()\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += self.calculate_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "            average_val_accuracy = val_accuracy / len(self.val_dataloader)\n",
    "            average_val_loss = val_loss / len(self.val_dataloader)\n",
    "            time_val = datetime.now() - t0_val\n",
    "\n",
    "            print(f'Average Loss:     {average_val_loss}')\n",
    "            print(f'Average Accuracy: {average_val_accuracy}')\n",
    "            print(f'Time Taken:       {time_val}\\n')\n",
    "\n",
    "            loss_dict['average training loss'].append(average_train_loss)\n",
    "            loss_dict['average validation loss'].append(average_val_loss)\n",
    "\n",
    "        print(f'Total training time: {datetime.now()-t0_train}')\n",
    "\n",
    "    def calculate_accuracy(self, preds, labels):\n",
    "        \"\"\" Calculate the accuracy of model predictions against true labels.\n",
    "\n",
    "        Parameters:\n",
    "            preds (np.array): The predicted label from the model\n",
    "            labels (np.array): The true label\n",
    "\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy as a percentage of the correct\n",
    "                predictions.\n",
    "        \"\"\"\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"Return the predicted probabilities of each class for input text.\n",
    "        \n",
    "        Parameters:\n",
    "            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n",
    "                the token IDs and attention masks for the text to perform\n",
    "                inference on.\n",
    "        \n",
    "        Returns:\n",
    "            probs (PyTorch.Tensor): A tensor containing the probability values\n",
    "                for each class as predicted by the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n",
    "                for t in batch)[:2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_token_ids, attention_mask=batch_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                #logits = self.model(batch_token_ids, batch_attention_mask)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "\n",
    "\n",
    "'''        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72cde3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f237a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652581ad",
   "metadata": {},
   "source": [
    "# Fine tune + ElaLora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf978ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peftNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading peft-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (1.9.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from peft) (0.33.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers->peft) (0.21.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "Downloading peft-0.16.0-py3-none-any.whl (472 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.16.0\n"
     ]
    }
   ],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef60ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ FineTuningPipeline (Modify ElaLoRA)\n",
    "\n",
    "from loralib.elalora import SVDLinear\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_linear_schedule_with_warmup)\n",
    "\n",
    "\n",
    "class FineTuningPipeline:\n",
    "    def __init__(self, dataset, tokenizer, model, optimizer,\n",
    "                 loss_function=nn.CrossEntropyLoss(), val_size=0.1,\n",
    "                 epochs=4, seed=42 , allocator=None):\n",
    "        \n",
    "        self.allocator = allocator\n",
    "        self.df_dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.val_size = val_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.set_seeds()\n",
    "        self.freeze_base_weights()  # add freeze gradient\n",
    "        \n",
    "        # tokenization + dataloaders\n",
    "        self.token_ids, self.attention_masks = self.tokenize_dataset()\n",
    "        self.train_dataloader, self.val_dataloader = self.create_dataloaders()\n",
    "\n",
    "        # üî¥ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ\n",
    "        self.configure_allocator()\n",
    "\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        self.fine_tune()\n",
    "\n",
    "    # freeze fradient update weight\n",
    "    def freeze_base_weights(self):\n",
    "        print(\"üîí Freezing base model weights (non-SVDLinear layers)...\")\n",
    "\n",
    "        from loralib.elalora import SVDLinear\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, SVDLinear):\n",
    "                for param_name, param in module.named_parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in module.parameters(recurse=False):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        print(\"üîé Checking which parameters are trainable...\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"‚úÖ TRAINING: {name}\")\n",
    "            else:\n",
    "                print(f\"‚ùå FROZEN:   {name}\")\n",
    "\n",
    "    '''\n",
    "    def tokenize(self, text):\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=256, # before 128 , 512 , 256\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded['input_ids'], encoded['attention_mask']\n",
    "    '''\n",
    "\n",
    "    def tokenize(self, text, max_len=256, ratio=0.72):\n",
    "        budget = max_len - 2\n",
    "        head = int(budget * ratio)\n",
    "        tail = budget - head\n",
    "\n",
    "        toks = self.tokenizer(text, add_special_tokens=False).input_ids\n",
    "        if len(toks) > budget:\n",
    "            toks = toks[:head] + toks[-tail:]\n",
    "\n",
    "        # ‡πÉ‡∏ä‡πâ return_tensors=None ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏≠‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏° shape ‡∏ä‡∏±‡∏ß‡∏£‡πå\n",
    "        enc = self.tokenizer.prepare_for_model(\n",
    "            toks, max_length=max_len, truncation=True, padding='max_length', return_tensors=None\n",
    "        )\n",
    "        ids  = torch.tensor(enc[\"input_ids\"], dtype=torch.long).unsqueeze(0)      # (1, L)\n",
    "        mask = torch.tensor(enc[\"attention_mask\"], dtype=torch.long).unsqueeze(0) # (1, L)\n",
    "        return ids, mask\n",
    "\n",
    "    '''\n",
    "    def tokenize_dataset(self):\n",
    "        token_ids, attention_masks = [], []\n",
    "        for review in self.df_dataset['review_cleaned']:\n",
    "            ids, mask = self.tokenize(review)\n",
    "            token_ids.append(ids)\n",
    "            attention_masks.append(mask)\n",
    "        return torch.cat(token_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "    '''\n",
    "\n",
    "    def tokenize_dataset(self):\n",
    "        token_ids, attention_masks = [], []\n",
    "        for review in self.df_dataset['review_cleaned']:\n",
    "            ids, mask = self.tokenize(review)        # ids, mask shape = (1, L)\n",
    "            token_ids.append(ids)\n",
    "            attention_masks.append(mask)\n",
    "        token_ids = torch.cat(token_ids, dim=0)          # (N, L)\n",
    "        attention_masks = torch.cat(attention_masks, 0)  # (N, L)\n",
    "\n",
    "        # debug safety\n",
    "        print(\"shapes:\", token_ids.shape, attention_masks.shape)\n",
    "        return token_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)\n",
    "        train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "            self.token_ids, \n",
    "            self.attention_masks, \n",
    "            labels, \n",
    "            test_size=self.val_size, \n",
    "            shuffle=True,\n",
    "            stratify=labels,\n",
    "            random_state=self.seed\n",
    "            )\n",
    "\n",
    "        train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "        val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "        #return DataLoader(train_data, shuffle=True, batch_size=16), DataLoader(val_data, batch_size=16)  # before bacth_size = 32\n",
    "\n",
    "        train_loader = DataLoader(train_data, shuffle=True,  batch_size=16, num_workers=2, pin_memory=True, drop_last=False)\n",
    "        val_loader   = DataLoader(val_data,   shuffle=False, batch_size=16, num_workers=2, pin_memory=True)\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def configure_allocator(self):\n",
    "        if self.allocator is None:\n",
    "            return\n",
    "        steps_per_epoch = len(self.train_dataloader)\n",
    "        total_steps = steps_per_epoch * self.epochs\n",
    "        # 10% / 60% / 10%\n",
    "        self.allocator.total_step    = total_steps\n",
    "        self.allocator.init_warmup   = int(0.10 * total_steps)\n",
    "        self.allocator.final_warmup  = int(0.10 * total_steps)\n",
    "        self.allocator.mask_interval = max(50, int(0.10 * total_steps))\n",
    "        print(\"Allocator:\", self.allocator.init_warmup, self.allocator.final_warmup,\n",
    "          self.allocator.mask_interval, self.allocator.total_step)\n",
    "\n",
    "    #def create_scheduler(self):\n",
    "    #    total_steps = self.epochs * len(self.train_dataloader)\n",
    "    #    return get_linear_schedule_with_warmup(self.optimizer, 0, total_steps)\n",
    "    \n",
    "    def create_scheduler(self):\n",
    "        total_steps = self.epochs * len(self.train_dataloader)\n",
    "        warmup_steps = int(0.10 * total_steps)  # 6% warmup\n",
    "        return get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "    def set_seeds(self):\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        from datetime import datetime\n",
    "        print(f\"üîç Model type: {type(self.model)}\")\n",
    "        t0_train = datetime.now()\n",
    "        global_step = 0  # üîÅ Step counter for ElaLoRA\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n===== Epoch {epoch+1}/{self.epochs} =====\")\n",
    "\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch in self.train_dataloader:\n",
    "                ids, mask, labels = [x.to(self.device) for x in batch]\n",
    "                self.model.zero_grad()\n",
    "                outputs = self.model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                # ‚úÖ NEW: Adapt rank if model supports it\n",
    "                if hasattr(self.model, \"maybe_adapt_rank\"):\n",
    "                    self.model.maybe_adapt_rank(global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"‚úÖ Avg Train Loss: {train_loss / len(self.train_dataloader):.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss, val_accuracy = 0, 0\n",
    "            t0_val = datetime.now()\n",
    "            for batch in self.val_dataloader:\n",
    "                ids, mask, labels = [x.to(self.device) for x in batch]\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += self.calculate_accuracy(logits.cpu().numpy(), labels.cpu().numpy())\n",
    "\n",
    "            val_time = datetime.now() - t0_val\n",
    "            print(f\"üß™ Avg Val Loss:  {val_loss / len(self.val_dataloader):.4f}\")\n",
    "            print(f\"üéØ Val Accuracy: {val_accuracy / len(self.val_dataloader):.4f}\")\n",
    "            print(f\"üïí Val Time:      {val_time}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Total training time: {datetime.now() - t0_train}\")\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        return np.sum(preds_flat == labels.flatten()) / len(labels)\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        \"\"\"Return the predicted probabilities of each class for input text.\n",
    "        \n",
    "        Parameters:\n",
    "            dataloader (torch.utils.data.DataLoader): A DataLoader containing\n",
    "                the token IDs and attention masks for the text to perform\n",
    "                inference on.\n",
    "        \n",
    "        Returns:\n",
    "            probs (PyTorch.Tensor): A tensor containing the probability values\n",
    "                for each class as predicted by the model.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \\\n",
    "                for t in batch)[:2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(batch_token_ids, attention_mask=batch_attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                #logits = self.model(batch_token_ids, batch_attention_mask)\n",
    "\n",
    "            all_logits.append(logits)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "        probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9281306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-win_amd64.whl (72.2 MB)\n",
      "   ---------------------------------------- 0.0/72.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.3/72.2 MB 9.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.4/72.2 MB 9.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 5.0/72.2 MB 8.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 6.8/72.2 MB 8.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 8.9/72.2 MB 9.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 11.0/72.2 MB 9.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 12.3/72.2 MB 8.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 14.4/72.2 MB 9.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 16.8/72.2 MB 9.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 18.4/72.2 MB 9.0 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 20.4/72.2 MB 9.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 22.8/72.2 MB 9.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 24.6/72.2 MB 9.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 26.5/72.2 MB 9.3 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 28.8/72.2 MB 9.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 30.4/72.2 MB 9.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 32.5/72.2 MB 9.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 34.6/72.2 MB 9.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 36.4/72.2 MB 9.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 38.3/72.2 MB 9.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 40.4/72.2 MB 9.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 42.2/72.2 MB 9.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 44.0/72.2 MB 9.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 46.1/72.2 MB 9.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 48.0/72.2 MB 9.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 50.1/72.2 MB 9.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 52.4/72.2 MB 9.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 54.3/72.2 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 56.4/72.2 MB 9.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 58.7/72.2 MB 9.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 60.8/72.2 MB 9.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 62.1/72.2 MB 9.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 63.4/72.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 65.8/72.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 67.6/72.2 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 69.2/72.2 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.3/72.2 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.2/72.2 MB 9.3 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e02d4",
   "metadata": {},
   "source": [
    "# ElaLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ElaLoRA' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Lenovo/Desktop/NLP/Final_project/test_run_model/ElaLoRA/loralib\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: loralib\n",
      "  Running setup.py develop for loralib\n",
      "Successfully installed loralib-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Legacy editable install of loralib==0.1.0 from file:///C:/Users/Lenovo/Desktop/NLP/Final_project/test_run_model/ElaLoRA/loralib (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "! git clone https://github.com/HuandongChang/ElaLoRA.git\n",
    "! pip install -e ./ElaLoRA/loralib\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271cf95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'List', 'LoRALayer', 'Optional', 'RankAllocator', 'SVDLinear', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'compute_orth_regu', 'json', 'math', 'nn', 'np', 'os', 'plot_ipt_graph', 'plot_rank', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import loralib.elalora\n",
    "print(dir(loralib.elalora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db246830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loralib.elalora import RankAllocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e991242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loralib.elalora import SVDLinear, RankAllocator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069840f2",
   "metadata": {},
   "source": [
    "# ElaLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf15dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.tok_embeddings.weight\n",
      "model.embeddings.norm.weight\n",
      "model.layers.0.attn.Wqkv.weight\n",
      "model.layers.0.attn.Wo.weight\n",
      "model.layers.0.mlp_norm.weight\n",
      "model.layers.0.mlp.Wi.weight\n",
      "model.layers.0.mlp.Wo.weight\n",
      "model.layers.1.attn_norm.weight\n",
      "model.layers.1.attn.Wqkv.weight\n",
      "model.layers.1.attn.Wo.weight\n",
      "model.layers.1.mlp_norm.weight\n",
      "model.layers.1.mlp.Wi.weight\n",
      "model.layers.1.mlp.Wo.weight\n",
      "model.layers.2.attn_norm.weight\n",
      "model.layers.2.attn.Wqkv.weight\n",
      "model.layers.2.attn.Wo.weight\n",
      "model.layers.2.mlp_norm.weight\n",
      "model.layers.2.mlp.Wi.weight\n",
      "model.layers.2.mlp.Wo.weight\n",
      "model.layers.3.attn_norm.weight\n",
      "model.layers.3.attn.Wqkv.weight\n",
      "model.layers.3.attn.Wo.weight\n",
      "model.layers.3.mlp_norm.weight\n",
      "model.layers.3.mlp.Wi.weight\n",
      "model.layers.3.mlp.Wo.weight\n",
      "model.layers.4.attn_norm.weight\n",
      "model.layers.4.attn.Wqkv.weight\n",
      "model.layers.4.attn.Wo.weight\n",
      "model.layers.4.mlp_norm.weight\n",
      "model.layers.4.mlp.Wi.weight\n",
      "model.layers.4.mlp.Wo.weight\n",
      "model.layers.5.attn_norm.weight\n",
      "model.layers.5.attn.Wqkv.weight\n",
      "model.layers.5.attn.Wo.weight\n",
      "model.layers.5.mlp_norm.weight\n",
      "model.layers.5.mlp.Wi.weight\n",
      "model.layers.5.mlp.Wo.weight\n",
      "model.layers.6.attn_norm.weight\n",
      "model.layers.6.attn.Wqkv.weight\n",
      "model.layers.6.attn.Wo.weight\n",
      "model.layers.6.mlp_norm.weight\n",
      "model.layers.6.mlp.Wi.weight\n",
      "model.layers.6.mlp.Wo.weight\n",
      "model.layers.7.attn_norm.weight\n",
      "model.layers.7.attn.Wqkv.weight\n",
      "model.layers.7.attn.Wo.weight\n",
      "model.layers.7.mlp_norm.weight\n",
      "model.layers.7.mlp.Wi.weight\n",
      "model.layers.7.mlp.Wo.weight\n",
      "model.layers.8.attn_norm.weight\n",
      "model.layers.8.attn.Wqkv.weight\n",
      "model.layers.8.attn.Wo.weight\n",
      "model.layers.8.mlp_norm.weight\n",
      "model.layers.8.mlp.Wi.weight\n",
      "model.layers.8.mlp.Wo.weight\n",
      "model.layers.9.attn_norm.weight\n",
      "model.layers.9.attn.Wqkv.weight\n",
      "model.layers.9.attn.Wo.weight\n",
      "model.layers.9.mlp_norm.weight\n",
      "model.layers.9.mlp.Wi.weight\n",
      "model.layers.9.mlp.Wo.weight\n",
      "model.layers.10.attn_norm.weight\n",
      "model.layers.10.attn.Wqkv.weight\n",
      "model.layers.10.attn.Wo.weight\n",
      "model.layers.10.mlp_norm.weight\n",
      "model.layers.10.mlp.Wi.weight\n",
      "model.layers.10.mlp.Wo.weight\n",
      "model.layers.11.attn_norm.weight\n",
      "model.layers.11.attn.Wqkv.weight\n",
      "model.layers.11.attn.Wo.weight\n",
      "model.layers.11.mlp_norm.weight\n",
      "model.layers.11.mlp.Wi.weight\n",
      "model.layers.11.mlp.Wo.weight\n",
      "model.layers.12.attn_norm.weight\n",
      "model.layers.12.attn.Wqkv.weight\n",
      "model.layers.12.attn.Wo.weight\n",
      "model.layers.12.mlp_norm.weight\n",
      "model.layers.12.mlp.Wi.weight\n",
      "model.layers.12.mlp.Wo.weight\n",
      "model.layers.13.attn_norm.weight\n",
      "model.layers.13.attn.Wqkv.weight\n",
      "model.layers.13.attn.Wo.weight\n",
      "model.layers.13.mlp_norm.weight\n",
      "model.layers.13.mlp.Wi.weight\n",
      "model.layers.13.mlp.Wo.weight\n",
      "model.layers.14.attn_norm.weight\n",
      "model.layers.14.attn.Wqkv.weight\n",
      "model.layers.14.attn.Wo.weight\n",
      "model.layers.14.mlp_norm.weight\n",
      "model.layers.14.mlp.Wi.weight\n",
      "model.layers.14.mlp.Wo.weight\n",
      "model.layers.15.attn_norm.weight\n",
      "model.layers.15.attn.Wqkv.weight\n",
      "model.layers.15.attn.Wo.weight\n",
      "model.layers.15.mlp_norm.weight\n",
      "model.layers.15.mlp.Wi.weight\n",
      "model.layers.15.mlp.Wo.weight\n",
      "model.layers.16.attn_norm.weight\n",
      "model.layers.16.attn.Wqkv.weight\n",
      "model.layers.16.attn.Wo.weight\n",
      "model.layers.16.mlp_norm.weight\n",
      "model.layers.16.mlp.Wi.weight\n",
      "model.layers.16.mlp.Wo.weight\n",
      "model.layers.17.attn_norm.weight\n",
      "model.layers.17.attn.Wqkv.weight\n",
      "model.layers.17.attn.Wo.weight\n",
      "model.layers.17.mlp_norm.weight\n",
      "model.layers.17.mlp.Wi.weight\n",
      "model.layers.17.mlp.Wo.weight\n",
      "model.layers.18.attn_norm.weight\n",
      "model.layers.18.attn.Wqkv.weight\n",
      "model.layers.18.attn.Wo.weight\n",
      "model.layers.18.mlp_norm.weight\n",
      "model.layers.18.mlp.Wi.weight\n",
      "model.layers.18.mlp.Wo.weight\n",
      "model.layers.19.attn_norm.weight\n",
      "model.layers.19.attn.Wqkv.weight\n",
      "model.layers.19.attn.Wo.weight\n",
      "model.layers.19.mlp_norm.weight\n",
      "model.layers.19.mlp.Wi.weight\n",
      "model.layers.19.mlp.Wo.weight\n",
      "model.layers.20.attn_norm.weight\n",
      "model.layers.20.attn.Wqkv.weight\n",
      "model.layers.20.attn.Wo.weight\n",
      "model.layers.20.mlp_norm.weight\n",
      "model.layers.20.mlp.Wi.weight\n",
      "model.layers.20.mlp.Wo.weight\n",
      "model.layers.21.attn_norm.weight\n",
      "model.layers.21.attn.Wqkv.weight\n",
      "model.layers.21.attn.Wo.weight\n",
      "model.layers.21.mlp_norm.weight\n",
      "model.layers.21.mlp.Wi.weight\n",
      "model.layers.21.mlp.Wo.weight\n",
      "model.final_norm.weight\n",
      "head.dense.weight\n",
      "head.norm.weight\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d3fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RankAllocator in module loralib.elalora:\n",
      "\n",
      "class RankAllocator(builtins.object)\n",
      " |  RankAllocator(model, lora_r: int, target_rank: int, init_warmup: int, final_warmup: int, mask_interval: int, beta1: float, beta2: float, total_step: Optional[int] = None, target_total_rank: Optional[int] = None, tb_writter=None, tb_writter_loginterval: int = 500, k: int = 2, b: int = 4, output_dir: str = None, enable_scheduler: bool = False)\n",
      " |\n",
      " |  The RankAllocator for AdaLoRA Model that will be called every training step.\n",
      " |  Paper: https://openreview.net/pdf?id=lq62uWRJjiY\n",
      " |\n",
      " |  Args:\n",
      " |      model: the model that we apply AdaLoRA to.\n",
      " |      lora_r (`int`): The initial rank for each incremental matrix.\n",
      " |      target_rank (`int`): The target average rank of incremental matrix.\n",
      " |      init_warmup (`int`): The steps of initial fine-tuning warmup.\n",
      " |      final_warmup (`int`): The step of final fine-tuning.\n",
      " |      mask_interval (`int`): The time internval between two budget allocations.\n",
      " |      beta1 (`float`): The hyperparameter of EMA for sensitivity smoothing.\n",
      " |      beta2 (`float`): The hyperparameter of EMA for undertainty quantification.\n",
      " |      total_step (`int`): The total training steps, correctly configured before training.\n",
      " |      target_total_rank (`Optinal[int]`): The speficified final total rank.\n",
      " |      tb_writter (`SummaryWriter`): Tensorboard SummaryWriter.\n",
      " |      tb_writter_loginterval (`int`): The logging interval of SummaryWriter.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, model, lora_r: int, target_rank: int, init_warmup: int, final_warmup: int, mask_interval: int, beta1: float, beta2: float, total_step: Optional[int] = None, target_total_rank: Optional[int] = None, tb_writter=None, tb_writter_loginterval: int = 500, k: int = 2, b: int = 4, output_dir: str = None, enable_scheduler: bool = False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  calculate_score(self, n, p=None, metric='ipt')\n",
      " |\n",
      " |  get_lora_param_name(self)\n",
      " |\n",
      " |  get_rank_pattern(self)\n",
      " |\n",
      " |  mask_to_target_rank(self, model, curr_rank)\n",
      " |\n",
      " |  set_total_step(self, total_step: int)\n",
      " |\n",
      " |  update_and_mask(self, model, global_step)\n",
      " |\n",
      " |  update_ipt(self, model)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from loralib.elalora import RankAllocator\n",
    "help(RankAllocator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Freezing base model weights (non-SVDLinear layers)...\n",
      "üîé Checking which parameters are trainable...\n",
      "‚ùå FROZEN:   model.embeddings.tok_embeddings.weight\n",
      "‚ùå FROZEN:   model.embeddings.norm.weight\n",
      "‚ùå FROZEN:   model.layers.0.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.0.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn_norm.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn.Wqkv.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp_norm.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp.Wi.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.final_norm.weight\n",
      "‚úÖ TRAINING: head.dense.weight\n",
      "‚ùå FROZEN:   head.norm.weight\n",
      "‚úÖ TRAINING: classifier.weight\n",
      "‚úÖ TRAINING: classifier.bias\n",
      "üîç Model type: <class 'transformers.models.modernbert.modeling_modernbert.ModernBertForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/3 =====\n",
      "‚úÖ Avg Train Loss: 0.4402\n",
      "üß™ Avg Val Loss:  0.3800\n",
      "üéØ Val Accuracy: 0.8253\n",
      "üïí Val Time:      0:01:11.920798\n",
      "\n",
      "===== Epoch 2/3 =====\n",
      "‚úÖ Avg Train Loss: 0.3718\n",
      "üß™ Avg Val Loss:  0.3500\n",
      "üéØ Val Accuracy: 0.8391\n",
      "üïí Val Time:      0:01:06.056607\n",
      "\n",
      "===== Epoch 3/3 =====\n",
      "‚úÖ Avg Train Loss: 0.3463\n",
      "üß™ Avg Val Loss:  0.3443\n",
      "üéØ Val Accuracy: 0.8480\n",
      "üïí Val Time:      0:00:57.156569\n",
      "\n",
      "‚úÖ Total training time: 0:33:43.763956\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from loralib.elalora import SVDLinear, RankAllocator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ‚úÖ ‡πÅ‡∏õ‡∏∞ SVDLinear ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï‡πÉ‡∏ô‡∏™‡∏≤‡∏¢ BERT/ModernBERT)\n",
    "#   - ‡∏ñ‡πâ‡∏≤ print(base_model) ‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠ q_proj/k_proj/... ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏ã‡πá‡∏ï‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"dense\",\"classifier\"}\n",
    "TARGET_LINEAR_TOKENS = {\"query\",\"key\",\"value\",\"dense\",\"intermediate\",\"output\",\"classifier\"}\n",
    "\n",
    "def replace_linear_with_svdlinear(module, prefix=\"\"):\n",
    "    for name, child in module.named_children():\n",
    "        full = f\"{prefix}.{name}\" if prefix else name\n",
    "        if isinstance(child, nn.Linear) and any(tok in full for tok in TARGET_LINEAR_TOKENS):\n",
    "            in_f, out_f, has_bias = child.in_features, child.out_features, child.bias is not None\n",
    "            setattr(module, name, SVDLinear(in_f, out_f, bias=has_bias))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(child, full)\n",
    "\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "# ‚úÖ Freeze base / train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ SVDLinear (+ head ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# ‚úÖ Optimizer: ‡∏Å‡∏±‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ã‡πâ‡∏≥‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á adapters ‡∏Å‡∏±‡∏ö head\n",
    "seen = set()\n",
    "\n",
    "adap_params = []\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            if p.requires_grad and id(p) not in seen:\n",
    "                adap_params.append(p)\n",
    "                seen.add(id(p))\n",
    "\n",
    "head_params = []\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        if p.requires_grad and id(p) not in seen:\n",
    "            head_params.append(p)\n",
    "            seen.add(id(p))\n",
    "\n",
    "param_groups = []\n",
    "if adap_params:\n",
    "    param_groups.append({\"params\": adap_params, \"lr\": 1.5e-3, \"weight_decay\": 0.01})\n",
    "if head_params:\n",
    "    param_groups.append({\"params\": head_params, \"lr\": 2e-3, \"weight_decay\": 0.0})\n",
    "\n",
    "optimizer = AdamW(param_groups)\n",
    "\n",
    "# ‚úÖ Steps (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏î‡πâ ‡∏ñ‡πâ‡∏≤ Pipeline ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á dataloader ‡πÄ‡∏≠‡∏á)\n",
    "epochs = 3\n",
    "estimated_total_steps = 2000  # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô len(train_loader)*epochs ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
    "\n",
    "allocator = RankAllocator(\n",
    "    model=base_model,\n",
    "    lora_r=16,\n",
    "    target_rank=24,\n",
    "    init_warmup=int(0.10 * estimated_total_steps),\n",
    "    final_warmup=int(0.60 * estimated_total_steps),\n",
    "    mask_interval=max(50, int(0.10 * estimated_total_steps)),\n",
    "    total_step=estimated_total_steps,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85\n",
    ")\n",
    "\n",
    "# ‚úÖ Fine-tuning pipeline (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô preprocess: max_length‚âà320) use 256\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=allocator\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7a0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVDLinear count = 90\n",
      "['model.layers.0.attn.Wqkv', 'model.layers.0.attn.Wo', 'model.layers.0.mlp.Wi', 'model.layers.0.mlp.Wo', 'model.layers.1.attn.Wqkv', 'model.layers.1.attn.Wo', 'model.layers.1.mlp.Wi', 'model.layers.1.mlp.Wo', 'model.layers.2.attn.Wqkv', 'model.layers.2.attn.Wo', 'model.layers.2.mlp.Wi', 'model.layers.2.mlp.Wo', 'model.layers.3.attn.Wqkv', 'model.layers.3.attn.Wo', 'model.layers.3.mlp.Wi', 'model.layers.3.mlp.Wo', 'model.layers.4.attn.Wqkv', 'model.layers.4.attn.Wo', 'model.layers.4.mlp.Wi', 'model.layers.4.mlp.Wo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Freezing base model weights (non-SVDLinear layers)...\n",
      "üîé Checking which parameters are trainable...\n",
      "‚ùå FROZEN:   model.embeddings.tok_embeddings.weight\n",
      "‚ùå FROZEN:   model.embeddings.norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.final_norm.weight\n",
      "‚úÖ TRAINING: head.dense.weight\n",
      "‚ùå FROZEN:   head.norm.weight\n",
      "‚úÖ TRAINING: classifier.weight\n",
      "‚úÖ TRAINING: classifier.bias\n",
      "shapes: torch.Size([50000, 256]) torch.Size([50000, 256])\n",
      "Allocator: 1406 1406 1406 14065\n",
      "üîç Model type: <class 'transformers.models.modernbert.modeling_modernbert.ModernBertForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/5 =====\n",
      "‚úÖ Avg Train Loss: 0.4861\n",
      "üß™ Avg Val Loss:  0.3406\n",
      "üéØ Val Accuracy: 0.8482\n",
      "üïí Val Time:      0:00:54.444112\n",
      "\n",
      "===== Epoch 2/5 =====\n",
      "‚úÖ Avg Train Loss: 0.3273\n",
      "üß™ Avg Val Loss:  0.3082\n",
      "üéØ Val Accuracy: 0.8752\n",
      "üïí Val Time:      0:00:53.906829\n",
      "\n",
      "===== Epoch 3/5 =====\n",
      "‚úÖ Avg Train Loss: 0.2826\n",
      "üß™ Avg Val Loss:  0.2914\n",
      "üéØ Val Accuracy: 0.8770\n",
      "üïí Val Time:      0:00:53.922269\n",
      "\n",
      "===== Epoch 4/5 =====\n",
      "‚úÖ Avg Train Loss: 0.2524\n",
      "üß™ Avg Val Loss:  0.2903\n",
      "üéØ Val Accuracy: 0.8806\n",
      "üïí Val Time:      0:01:15.961936\n",
      "\n",
      "===== Epoch 5/5 =====\n",
      "‚úÖ Avg Train Loss: 0.2228\n",
      "üß™ Avg Val Loss:  0.2935\n",
      "üéØ Val Accuracy: 0.8790\n",
      "üïí Val Time:      0:00:53.975584\n",
      "\n",
      "‚úÖ Total training time: 2:18:30.851681\n"
     ]
    }
   ],
   "source": [
    "from loralib.elalora import SVDLinear, RankAllocator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# SVDLinear  BERT/ModernBERT\n",
    "#   - ‡∏ñ‡πâ‡∏≤ print(base_model) ‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠ q_proj/k_proj/... ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏ã‡πá‡∏ï‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"dense\",\"classifier\"}\n",
    "#TARGET_LINEAR_TOKENS = {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"dense\",\"classifier\"}\n",
    "#TARGET_LINEAR_TOKENS = {\"query\",\"key\",\"value\",\"dense\",\"intermediate\",\"output\",\"classifier\"}\n",
    "TARGET_LINEAR_TOKENS = {\"Wqkv\", \"Wo\", \"Wi\",\"dense\",\"classifier\"}\n",
    "\n",
    "def replace_linear_with_svdlinear(module, prefix=\"\"):\n",
    "    for name, child in module.named_children():\n",
    "        full = f\"{prefix}.{name}\" if prefix else name\n",
    "        if isinstance(child, nn.Linear) and any(tok in full for tok in TARGET_LINEAR_TOKENS):\n",
    "            in_f, out_f, has_bias = child.in_features, child.out_features, child.bias is not None\n",
    "            setattr(module, name, SVDLinear(in_f, out_f, bias=has_bias))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(child, full)\n",
    "\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "from loralib.elalora import SVDLinear\n",
    "svd_names = [n for n, m in base_model.named_modules() if isinstance(m, SVDLinear)]\n",
    "print(\"SVDLinear count =\", len(svd_names))\n",
    "print(svd_names[:20])\n",
    "\n",
    "#  Freeze base / train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ SVDLinear \n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# ‡πÄ‡∏õ‡∏¥‡∏î classifier ‡πÄ\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡πÄ‡∏õ‡∏¥‡∏î‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ \"norm\"\n",
    "for name, module in base_model.named_modules():\n",
    "    if name.endswith(\"norm\"):\n",
    "        for p in module.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "# ‚úÖ Optimizer: adapters ‡∏Å‡∏±‡∏ö head\n",
    "seen = set()\n",
    "\n",
    "adap_params = []\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            if p.requires_grad and id(p) not in seen:\n",
    "                adap_params.append(p)\n",
    "                seen.add(id(p))\n",
    "\n",
    "head_params = []\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        if p.requires_grad and id(p) not in seen:\n",
    "            head_params.append(p)\n",
    "            seen.add(id(p))\n",
    "\n",
    "param_groups = []\n",
    "if adap_params:\n",
    "    param_groups.append({\"params\": adap_params, \"lr\": 1.5e-3, \"weight_decay\": 0.01})\n",
    "if head_params:\n",
    "    param_groups.append({\"params\": head_params, \"lr\": 2.0e-3, \"weight_decay\": 0.0})\n",
    "\n",
    "optimizer = AdamW(param_groups)\n",
    "\n",
    "# ‚úÖ Steps \n",
    "epochs = 5\n",
    "'''\n",
    "tmp_pipeline = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=None  # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡πà‡∏á allocator ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ\n",
    ")\n",
    "'''\n",
    "\n",
    "#steps_per_epoch = len(tmp_pipeline.train_dataloader)\n",
    "#estimated_total_steps = steps_per_epoch * epochs\n",
    "\n",
    "allocator = RankAllocator(\n",
    "    model=base_model,\n",
    "    lora_r=24,\n",
    "    target_rank=24,\n",
    "    init_warmup=1,         # dummy, ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å override ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô pipeline\n",
    "    final_warmup=1,        # dummy\n",
    "    mask_interval=1,       # dummy\n",
    "    total_step=1,\n",
    "    #init_warmup=int(0.10 * estimated_total_steps),\n",
    "    #final_warmup=int(0.60 * estimated_total_steps),\n",
    "    #mask_interval=max(50, int(0.10 * estimated_total_steps)),\n",
    "    #total_step=estimated_total_steps,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "# ‚úÖ Fine-tuning pipeline  use 320\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_fn,   \n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=allocator\n",
    ")\n",
    "\n",
    "### adjust code ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db030b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVDLinear count = 90\n",
      "['model.layers.0.attn.Wqkv', 'model.layers.0.attn.Wo', 'model.layers.0.mlp.Wi', 'model.layers.0.mlp.Wo', 'model.layers.1.attn.Wqkv', 'model.layers.1.attn.Wo', 'model.layers.1.mlp.Wi', 'model.layers.1.mlp.Wo', 'model.layers.2.attn.Wqkv', 'model.layers.2.attn.Wo', 'model.layers.2.mlp.Wi', 'model.layers.2.mlp.Wo', 'model.layers.3.attn.Wqkv', 'model.layers.3.attn.Wo', 'model.layers.3.mlp.Wi', 'model.layers.3.mlp.Wo', 'model.layers.4.attn.Wqkv', 'model.layers.4.attn.Wo', 'model.layers.4.mlp.Wi', 'model.layers.4.mlp.Wo']\n",
      "üîí Freezing base model weights (non-SVDLinear layers)...\n",
      "üîé Checking which parameters are trainable...\n",
      "‚ùå FROZEN:   model.embeddings.tok_embeddings.weight\n",
      "‚ùå FROZEN:   model.embeddings.norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.final_norm.weight\n",
      "‚úÖ TRAINING: head.dense.weight\n",
      "‚ùå FROZEN:   head.norm.weight\n",
      "‚úÖ TRAINING: classifier.weight\n",
      "‚úÖ TRAINING: classifier.bias\n",
      "üîç Model type: <class 'transformers.models.modernbert.modeling_modernbert.ModernBertForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/3 =====\n",
      "‚úÖ Avg Train Loss: 0.4253\n",
      "üß™ Avg Val Loss:  0.3280\n",
      "üéØ Val Accuracy: 0.8616\n",
      "üïí Val Time:      0:01:16.103093\n",
      "\n",
      "===== Epoch 2/3 =====\n",
      "‚úÖ Avg Train Loss: 0.3163\n",
      "üß™ Avg Val Loss:  0.3079\n",
      "üéØ Val Accuracy: 0.8704\n",
      "üïí Val Time:      0:00:51.042658\n",
      "\n",
      "===== Epoch 3/3 =====\n",
      "‚úÖ Avg Train Loss: 0.2651\n",
      "üß™ Avg Val Loss:  0.3089\n",
      "üéØ Val Accuracy: 0.8764\n",
      "üïí Val Time:      0:00:52.466242\n",
      "\n",
      "‚úÖ Total training time: 1:41:20.580435\n",
      "üîí Freezing base model weights (non-SVDLinear layers)...\n",
      "üîé Checking which parameters are trainable...\n",
      "‚ùå FROZEN:   model.embeddings.tok_embeddings.weight\n",
      "‚ùå FROZEN:   model.embeddings.norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.final_norm.weight\n",
      "‚úÖ TRAINING: head.dense.weight\n",
      "‚ùå FROZEN:   head.norm.weight\n",
      "‚úÖ TRAINING: classifier.weight\n",
      "‚úÖ TRAINING: classifier.bias\n",
      "Allocator: 843 5063 843 8439\n",
      "üîç Model type: <class 'transformers.models.modernbert.modeling_modernbert.ModernBertForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/3 =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# ‚úÖ Fine-tuning pipeline (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô preprocess: max_length‚âà320) use 320\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m fine_tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mFineTuningPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallocator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallocator\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mFineTuningPipeline.__init__\u001b[1;34m(self, dataset, tokenizer, model, optimizer, loss_function, val_size, epochs, seed, allocator)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigure_allocator()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_scheduler()\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m, in \u001b[0;36mFineTuningPipeline.fine_tune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    152\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 153\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from loralib.elalora import SVDLinear, RankAllocator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ‚úÖ ‡πÅ‡∏õ‡∏∞ SVDLinear ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï‡πÉ‡∏ô‡∏™‡∏≤‡∏¢ BERT/ModernBERT)\n",
    "#   - ‡∏ñ‡πâ‡∏≤ print(base_model) ‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠ q_proj/k_proj/... ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏ã‡πá‡∏ï‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"dense\",\"classifier\"}\n",
    "#TARGET_LINEAR_TOKENS = {\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"dense\",\"classifier\"}\n",
    "#TARGET_LINEAR_TOKENS = {\"query\",\"key\",\"value\",\"dense\",\"intermediate\",\"output\",\"classifier\"}\n",
    "TARGET_LINEAR_TOKENS = {\"Wqkv\", \"Wo\", \"Wi\", \"dense\", \"classifier\"}\n",
    "\n",
    "def replace_linear_with_svdlinear(module, prefix=\"\"):\n",
    "    for name, child in module.named_children():\n",
    "        full = f\"{prefix}.{name}\" if prefix else name\n",
    "        if isinstance(child, nn.Linear) and any(tok in full for tok in TARGET_LINEAR_TOKENS):\n",
    "            in_f, out_f, has_bias = child.in_features, child.out_features, child.bias is not None\n",
    "            setattr(module, name, SVDLinear(in_f, out_f, bias=has_bias))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(child, full)\n",
    "\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "from loralib.elalora import SVDLinear\n",
    "svd_names = [n for n, m in base_model.named_modules() if isinstance(m, SVDLinear)]\n",
    "print(\"SVDLinear count =\", len(svd_names))\n",
    "print(svd_names[:20])\n",
    "\n",
    "# ‚úÖ Freeze base / train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ SVDLinear (+ head ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# ‚úÖ Optimizer: ‡∏Å‡∏±‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ã‡πâ‡∏≥‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á adapters ‡∏Å‡∏±‡∏ö head\n",
    "seen = set()\n",
    "\n",
    "adap_params = []\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            if p.requires_grad and id(p) not in seen:\n",
    "                adap_params.append(p)\n",
    "                seen.add(id(p))\n",
    "\n",
    "head_params = []\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        if p.requires_grad and id(p) not in seen:\n",
    "            head_params.append(p)\n",
    "            seen.add(id(p))\n",
    "\n",
    "param_groups = []\n",
    "if adap_params:\n",
    "    param_groups.append({\"params\": adap_params, \"lr\": 5e-4, \"weight_decay\": 0.01})\n",
    "if head_params:\n",
    "    param_groups.append({\"params\": head_params, \"lr\": 1e-3, \"weight_decay\": 0.0})\n",
    "\n",
    "optimizer = AdamW(param_groups)\n",
    "\n",
    "# ‚úÖ Steps (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏î‡πâ ‡∏ñ‡πâ‡∏≤ Pipeline ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á dataloader ‡πÄ‡∏≠‡∏á)\n",
    "epochs = 3\n",
    "'''\n",
    "tmp_pipeline = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=None  # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡πà‡∏á allocator ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ\n",
    ")\n",
    "'''\n",
    "\n",
    "#steps_per_epoch = len(tmp_pipeline.train_dataloader)\n",
    "#estimated_total_steps = steps_per_epoch * epochs\n",
    "\n",
    "allocator = RankAllocator(\n",
    "    model=base_model,\n",
    "    lora_r=32,\n",
    "    target_rank=32,\n",
    "    init_warmup=1,         # dummy, ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å override ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô pipeline\n",
    "    final_warmup=1,        # dummy\n",
    "    mask_interval=1,       # dummy\n",
    "    total_step=1,\n",
    "    #init_warmup=int(0.10 * estimated_total_steps),\n",
    "    #final_warmup=int(0.60 * estimated_total_steps),\n",
    "    #mask_interval=max(50, int(0.10 * estimated_total_steps)),\n",
    "    #total_step=estimated_total_steps,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "# ‚úÖ Fine-tuning pipeline (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô preprocess: max_length‚âà320) use 320\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_fn,   \n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=allocator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# ... ‡∏™‡∏£‡πâ‡∏≤‡∏á train_dataloader / val_dataloader ‡πÅ‡∏•‡πâ‡∏ß ...\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m)) \u001b[38;5;241m*\u001b[39m epochs\n\u001b[0;32m     31\u001b[0m allocator \u001b[38;5;241m=\u001b[39m RankAllocator(\n\u001b[0;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[0;32m     33\u001b[0m     lora_r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     beta1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, beta2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m adap_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mmodules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, SVDLinear) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mparameters()]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from loralib.elalora import SVDLinear, RankAllocator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "# ‚úÖ 1) ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡πÅ‡∏•‡∏∞ base model\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ‚úÖ 2) ‡πÅ‡∏õ‡∏∞ SVDLinear ‡πÅ‡∏ó‡∏ô Linear ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "def replace_linear_with_svdlinear(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, SVDLinear(module.in_features, module.out_features, bias=module.bias is not None))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(module)\n",
    "\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "# ‚úÖ 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á RankAllocator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dynamic rank control\n",
    "from loralib.elalora import RankAllocator\n",
    "import math\n",
    "\n",
    "epochs = 3\n",
    "# ... ‡∏™‡∏£‡πâ‡∏≤‡∏á train_dataloader / val_dataloader ‡πÅ‡∏•‡πâ‡∏ß ...\n",
    "steps = math.ceil(len(train_dataloader)) * epochs\n",
    "\n",
    "allocator = RankAllocator(\n",
    "    model=base_model,\n",
    "    lora_r=16,\n",
    "    target_rank=24,\n",
    "    init_warmup=int(0.10*steps),\n",
    "    final_warmup=int(0.60*steps),\n",
    "    mask_interval=max(50, int(0.10*steps)),\n",
    "    total_step=steps,\n",
    "    beta1=0.85, beta2=0.85\n",
    ")\n",
    "\n",
    "adap_params = [p for m in base_model.modules() if isinstance(m, SVDLinear) for p in m.parameters()]\n",
    "head_params = list(base_model.classifier.parameters()) if hasattr(base_model, \"classifier\") else []\n",
    "optimizer = AdamW([\n",
    "    {\"params\": adap_params, \"lr\": 1.5e-3, \"weight_decay\": 0.01},\n",
    "    {\"params\": head_params, \"lr\": 2e-3, \"weight_decay\": 0.0},\n",
    "])\n",
    "\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,          # ‡πÉ‡∏ô preprocess ‡πÉ‡∏™‡πà max_length=320 ‡πÅ‡∏•‡πâ‡∏ß\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=allocator\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d43ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "some parameters appear in more than one parameter group",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m adap_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mmodules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, SVDLinear) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[0;32m     40\u001b[0m head_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(base_model\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(base_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m---> 41\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43madap_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ‚úÖ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡πÄ‡∏ï‡πá‡∏õ (‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ batch_size ‡πÉ‡∏ô Pipeline ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏ï‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\optim\\adamw.py:77\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     66\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     67\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:371\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:1050\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[1;34m(self, param_group)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     param_set\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mset\u001b[39m(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param_set\u001b[38;5;241m.\u001b[39misdisjoint(\u001b[38;5;28mset\u001b[39m(param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m-> 1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome parameters appear in more than one parameter group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\u001b[38;5;241m.\u001b[39mappend(param_group)\n",
      "\u001b[1;31mValueError\u001b[0m: some parameters appear in more than one parameter group"
     ]
    }
   ],
   "source": [
    "from loralib.elalora import SVDLinear, RankAllocator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ‚úÖ ‡πÅ‡∏õ‡∏∞ SVDLinear ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡πÑ‡∏°‡πà‡πÅ‡∏ó‡∏ô‡∏ó‡∏∏‡∏Å Linear)\n",
    "TARGET_LINEAR_NAMES = {\"Wqkv\", \"Wo\", \"Wi\", \"dense\", \"classifier\"}\n",
    "def replace_linear_with_svdlinear(module, prefix=\"\"):\n",
    "    for name, child in module.named_children():\n",
    "        full = f\"{prefix}.{name}\" if prefix else name\n",
    "        if isinstance(child, nn.Linear) and any(n in full.split('.') for n in TARGET_LINEAR_NAMES):\n",
    "            in_f, out_f, has_bias = child.in_features, child.out_features, child.bias is not None\n",
    "            setattr(module, name, SVDLinear(in_f, out_f, bias=has_bias))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(child, full)\n",
    "\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "# ‚úÖ Freeze base / train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ SVDLinear + classifier\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "for m in base_model.modules():\n",
    "    if isinstance(m, SVDLinear):\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "# ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ö‡∏≤‡∏á‡∏£‡∏∏‡πà‡∏ô head ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà classifier ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ù‡∏∂‡∏Å head ‡∏î‡πâ‡∏ß‡∏¢\n",
    "if hasattr(base_model, \"classifier\"):\n",
    "    for p in base_model.classifier.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# ‚úÖ Optimizer: LR ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö adapters/head\n",
    "adap_params = [p for m in base_model.modules() if isinstance(m, SVDLinear) for p in m.parameters()]\n",
    "head_params = list(base_model.classifier.parameters()) if hasattr(base_model, \"classifier\") else []\n",
    "optimizer = AdamW([\n",
    "    {\"params\": adap_params, \"lr\": 1.5e-3, \"weight_decay\": 0.01},\n",
    "    {\"params\": head_params, \"lr\": 2e-3, \"weight_decay\": 0.00},\n",
    "])\n",
    "\n",
    "# ‚úÖ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡πÄ‡∏ï‡πá‡∏õ (‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ batch_size ‡πÉ‡∏ô Pipeline ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏ï‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á)\n",
    "epochs = 3\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ batch_size/steps ‡∏Ç‡∏≠‡∏á Pipeline ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 2000\n",
    "estimated_total_steps = 2000\n",
    "\n",
    "allocator = RankAllocator(\n",
    "    model=base_model,\n",
    "    lora_r=16,                 # ‡πÄ‡∏î‡∏¥‡∏° 8 ‚Üí 16\n",
    "    target_rank=24,            # ‡πÄ‡∏î‡∏¥‡∏° 4 ‚Üí 24 (‡∏ñ‡πâ‡∏≤‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡πà‡∏≠‡∏¢‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô 16)\n",
    "    init_warmup=int(0.10*estimated_total_steps),\n",
    "    final_warmup=int(0.60*estimated_total_steps),\n",
    "    mask_interval=max(50, int(0.10*estimated_total_steps)),\n",
    "    total_step=estimated_total_steps,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85\n",
    ")\n",
    "\n",
    "# ‚úÖ Fine-tuning pipeline (‡πÄ‡∏û‡∏¥‡πà‡∏° epochs)\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,       # (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡πÅ‡∏Å‡πâ‡πÉ‡∏ô preprocess ‡πÄ‡∏õ‡πá‡∏ô max_length‚âà320)\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=epochs,\n",
    "    seed=42,\n",
    "    allocator=allocator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9bf6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå FROZEN: model.embeddings.tok_embeddings.weight\n",
      "‚ùå FROZEN: model.embeddings.norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.0.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.1.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.1.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.2.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.2.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.3.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.3.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.4.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.4.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.5.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.5.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.6.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.6.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.7.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.7.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.8.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.8.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.9.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.9.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.10.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.10.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.11.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.11.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.12.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.12.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.13.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.13.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.14.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.14.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.15.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.15.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.16.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.16.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.17.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.17.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.18.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.18.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.19.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.19.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.20.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.20.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.layers.21.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wo.weight\n",
      "‚ùå FROZEN: model.layers.21.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wo.weight\n",
      "‚ùå FROZEN: model.final_norm.weight\n",
      "‚úÖ TRAINING: head.dense.weight\n",
      "‚ùå FROZEN: head.norm.weight\n",
      "‚úÖ TRAINING: classifier.weight\n",
      "‚úÖ TRAINING: classifier.bias\n",
      "üîí Freezing base model weights (non-SVDLinear layers)...\n",
      "üîé Checking which parameters are trainable...\n",
      "‚ùå FROZEN:   model.embeddings.tok_embeddings.weight\n",
      "‚ùå FROZEN:   model.embeddings.norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.0.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.0.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.0.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.1.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.1.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.1.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.2.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.2.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.2.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.3.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.3.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.3.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.4.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.4.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.4.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.5.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.5.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.5.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.6.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.6.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.6.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.7.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.7.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.7.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.8.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.8.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.8.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.9.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.9.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.9.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.10.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.10.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.10.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.11.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.11.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.11.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.12.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.12.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.12.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.13.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.13.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.13.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.14.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.14.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.14.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.15.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.15.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.15.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.16.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.16.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.16.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.17.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.17.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.17.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.18.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.18.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.18.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.19.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.19.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.19.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.20.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.20.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.20.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.attn_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wqkv.weight\n",
      "‚úÖ TRAINING: model.layers.21.attn.Wo.weight\n",
      "‚ùå FROZEN:   model.layers.21.mlp_norm.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wi.weight\n",
      "‚úÖ TRAINING: model.layers.21.mlp.Wo.weight\n",
      "‚ùå FROZEN:   model.final_norm.weight\n",
      "‚úÖ TRAINING: head.dense.weight\n",
      "‚ùå FROZEN:   head.norm.weight\n",
      "‚úÖ TRAINING: classifier.weight\n",
      "‚úÖ TRAINING: classifier.bias\n",
      "üîç Model type: <class 'transformers.models.modernbert.modeling_modernbert.ModernBertForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/1 =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# ‚úÖ 6) Fine-tuning pipeline\u001b[39;00m\n\u001b[0;32m     54\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, base_model\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m fine_tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mFineTuningPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallocator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallocator\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# << ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ pipeline ‡∏ñ‡πâ‡∏≤ custom pipeline ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mFineTuningPipeline.__init__\u001b[1;34m(self, dataset, tokenizer, model, optimizer, loss_function, val_size, epochs, seed, allocator)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_dataloaders()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_scheduler()\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mFineTuningPipeline.fine_tune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    121\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m--> 122\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    124\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from loralib.elalora import SVDLinear, RankAllocator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "# ‚úÖ 1) ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡πÅ‡∏•‡∏∞ base model\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ‚úÖ 2) ‡πÅ‡∏õ‡∏∞ SVDLinear ‡πÅ‡∏ó‡∏ô Linear ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "def replace_linear_with_svdlinear(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, SVDLinear(module.in_features, module.out_features, bias=module.bias is not None))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(module)\n",
    "\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "# ‚úÖ 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á RankAllocator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dynamic rank control\n",
    "from loralib.elalora import RankAllocator\n",
    "\n",
    "#epochs = 3 # ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epoch ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "\n",
    "allocator = RankAllocator(\n",
    "    model=base_model,\n",
    "    lora_r=8,\n",
    "    target_rank=4,\n",
    "    init_warmup=100,\n",
    "    final_warmup=300,\n",
    "    mask_interval=50,\n",
    "    total_step=1000,         # ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å len(dataloader) x epochs ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ before 1000\n",
    "    beta1=0.9,\n",
    "    beta2=0.99\n",
    ")\n",
    "\n",
    "# ‚úÖ 4) ‡∏õ‡∏¥‡∏î gradients ‡∏Ç‡∏≠‡∏á base weights (freeze base model)\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for module in base_model.modules():\n",
    "    if isinstance(module, SVDLinear):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# ‚úÖ 5) ‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤ train ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ adapter ‡∏à‡∏£‡∏¥‡∏á\n",
    "for name, param in base_model.named_parameters():\n",
    "    print(\"‚úÖ TRAINING:\" if param.requires_grad else \"‚ùå FROZEN:\", name)\n",
    "\n",
    "# ‚úÖ 6) Fine-tuning pipeline\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, base_model.parameters()), lr=2e-5)\n",
    "\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=1,\n",
    "    seed=42,\n",
    "    allocator=allocator   # << ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ pipeline ‡∏ñ‡πâ‡∏≤ custom pipeline ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from loralib.elalora import ElaLoRAConfig\n",
    "from peft import get_peft_model  # PeftModel hooking\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 1) ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡πÅ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• base\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î config ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ElaLoRA (dynamic rank)\n",
    "elalora_config = ElaLoRAConfig(\n",
    "    init_oracle_rank=8,\n",
    "    mask_interval=100,       # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô steps ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£ prune/expand\n",
    "    prune_threshold=0.1,     # prune rank ‡∏ñ‡πâ‡∏≤ importance ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 10%\n",
    "    expand_rate=1.25,        # ‡πÄ‡∏û‡∏¥‡πà‡∏° rank 25% ‡πÄ‡∏°‡∏∑‡πà‡∏≠ layer ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "    apply_modules=[\"query\", \"key\", \"value\", \"intermediate\", \"output\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 3) ‡∏ú‡∏π‡∏Å ElaLoRA ‡πÄ‡∏Ç‡πâ‡∏≤ model\n",
    "model = elalora_config.apply_to(base_model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "fine_tuned_model.train()\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae817a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60978deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Model type: <class 'peft.peft_model.PeftModelForSequenceClassification'>\n",
      "\n",
      "===== Epoch 1/5 =====\n",
      "‚úÖ Avg Train Loss: 0.2423\n",
      "üß™ Avg Val Loss:  0.2054\n",
      "üéØ Val Accuracy: 0.9311\n",
      "üïí Val Time:      0:01:06.969529\n",
      "\n",
      "===== Epoch 2/5 =====\n",
      "‚úÖ Avg Train Loss: 0.1615\n",
      "üß™ Avg Val Loss:  0.1600\n",
      "üéØ Val Accuracy: 0.9409\n",
      "üïí Val Time:      0:01:07.422809\n",
      "\n",
      "===== Epoch 3/5 =====\n",
      "‚úÖ Avg Train Loss: 0.1414\n",
      "üß™ Avg Val Loss:  0.1717\n",
      "üéØ Val Accuracy: 0.9433\n",
      "üïí Val Time:      0:01:08.518395\n",
      "\n",
      "===== Epoch 4/5 =====\n",
      "‚úÖ Avg Train Loss: 0.1252\n",
      "üß™ Avg Val Loss:  0.1786\n",
      "üéØ Val Accuracy: 0.9437\n",
      "üïí Val Time:      0:01:07.966728\n",
      "\n",
      "===== Epoch 5/5 =====\n",
      "‚úÖ Avg Train Loss: 0.1117\n",
      "üß™ Avg Val Loss:  0.1851\n",
      "üéØ Val Accuracy: 0.9441\n",
      "üïí Val Time:      0:01:32.393248\n",
      "\n",
      "‚úÖ Total training time: 2:18:17.723577\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "\n",
    "# ‚úÖ 1) ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏°‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ModernBERT (DeBERTa-v3)\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "\n",
    "# ‚úÖ 2) ‡πÉ‡∏ä‡πâ dataset ‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß train ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "dataset = preprocess_dataset('C:/Users/Lenovo/Desktop/NLP/Final_project/IMDB Dataset.csv')\n",
    "# Use all 50,000 row data\n",
    "#dataset = dataset.sample(25000, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", num_labels=2)\n",
    "\n",
    "\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    #target_modules=[\"query\", \"value\"],\n",
    "    target_modules = ['dense', 'Wqkv', 'Wo', 'Wi'], \n",
    "    #target_modules = linear_names,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• LoRA\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "fine_tuned_model = FineTuningPipeline(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    val_size=0.1,\n",
    "    epochs=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4caa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "BATCH_SIZE = 8  # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° VRAM\n",
    "\n",
    "# ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå (GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ä‡πâ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# ‚úÖ ‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î CSV\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Desktop/NLP/Final_project/unseen_review/review.csv\")\n",
    "\n",
    "\n",
    "all_pred_labels = []\n",
    "\n",
    "# ‚úÖ predict ‡πÅ‡∏ö‡∏ö batch\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "    batch_texts = df[\"review\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    \n",
    "    # ‚úÖ tokenize + ‡∏™‡πà‡∏á‡πÑ‡∏õ GPU\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # ‚úÖ predict\n",
    "    with torch.no_grad():  \n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        batch_preds = probs.argmax(axis=1).cpu().numpy()  # ‚úÖ ‡∏Å‡∏•‡∏±‡∏ö CPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö\n",
    "    \n",
    "    all_pred_labels.extend(batch_preds)\n",
    "\n",
    "# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏Ç‡πâ‡∏≤ DataFrame\n",
    "df[\"predicted_label\"] = all_pred_labels\n",
    "df[\"predicted_sentiment\"] = [\"positive\" if p == 1 else \"negative\" for p in all_pred_labels]\n",
    "\n",
    "# ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á label ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô 0/1\n",
    "if \"sentiment\" in df.columns:  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ label ‡∏à‡∏£‡∏¥‡∏á\n",
    "    if df[\"sentiment\"].dtype == object:  \n",
    "        df[\"true_label\"] = df[\"sentiment\"].apply(lambda x: 1 if x.lower()==\"positive\" else 0)\n",
    "    else:\n",
    "        df[\"true_label\"] = df[\"sentiment\"]\n",
    "\n",
    "    # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Accuracy\n",
    "    acc = accuracy_score(df[\"true_label\"], df[\"predicted_label\"])\n",
    "    print(f\"\\n Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    # ‚úÖ Precision / Recall / F1\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(df[\"true_label\"], df[\"predicted_label\"], target_names=[\"negative\", \"positive\"]))\n",
    "else:\n",
    "    print(\"\\n No ground-truth labels found, skipping accuracy calculation\")\n",
    "\n",
    "# ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 10 row ‡πÅ‡∏£‡∏Å\n",
    "print(\"\\n=== Sample Predictions (first 10) ===\")\n",
    "for _, row in df.head(10).iterrows():\n",
    "    print(f\"Review: {row['review'][:80]}...\")\n",
    "    if \"sentiment\" in df.columns:\n",
    "        print(f\"  ‚úÖ True: {row['sentiment']} | üîÆ Predicted: {row['predicted_sentiment']}\\n\")\n",
    "    else:\n",
    "        print(f\"  üîÆ Predicted: {row['predicted_sentiment']}\\n\")\n",
    "\n",
    "# ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "#df.to_csv(\"C:/Users/Lenovo/Desktop/NLP/Final_project/prediction_result/reviews_with_predictions_Deberta_Lora_mode_full.csv\", index=False)\n",
    "#print(\"\\n‚úÖ Saved predictions to reviews_with_predictions_Deberta_Lora_mode_full.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14973dc5",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b10102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_deberta_Lora_model\\\\tokenizer_config.json',\n",
       " 'merged_deberta_Lora_model\\\\special_tokens_map.json',\n",
       " 'merged_deberta_Lora_model\\\\spm.model',\n",
       " 'merged_deberta_Lora_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_deberta_Lora_model\")\n",
    "tokenizer.save_pretrained(\"merged_deberta_Lora_model\")\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f381a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ModernBERT_ElaLora_full_svd_1\\\\tokenizer_config.json',\n",
       " './ModernBERT_ElaLora_full_svd_1\\\\special_tokens_map.json',\n",
       " './ModernBERT_ElaLora_full_svd_1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.save_pretrained(\"./ModernBERT_ElaLora_full_svd_1\")\n",
    "tokenizer.save_pretrained(\"./ModernBERT_ElaLora_full_svd_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837a939",
   "metadata": {},
   "source": [
    "# Import model fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f621abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "path = \"./ModernBERT_ElaLora_full_svd_1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0884c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e95c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e2b661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertForSequenceClassification(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): SVDLinear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): SVDLinear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): SVDLinear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): SVDLinear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-21): 21 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): SVDLinear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertRotaryEmbedding()\n",
       "          (Wo): SVDLinear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): SVDLinear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): SVDLinear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): SVDLinear(in_features=768, out_features=768, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): SVDLinear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from loralib.elalora import SVDLinear\n",
    "import torch.nn as nn\n",
    "\n",
    "# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏∞ SVDLinear ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô train\n",
    "def replace_linear_with_svdlinear(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, SVDLinear(module.in_features, module.out_features, bias=module.bias is not None))\n",
    "        else:\n",
    "            replace_linear_with_svdlinear(module)\n",
    "\n",
    "# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./ModernBERT_ElaLora_full\")\n",
    "\n",
    "# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î base model (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ SVDLinear)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./ModernBERT_ElaLora_full\", num_labels=2)\n",
    "\n",
    "# ‚úÖ ‡πÅ‡∏õ‡∏∞ SVDLinear ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î weights\n",
    "replace_linear_with_svdlinear(base_model)\n",
    "\n",
    "# üí• ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ base_model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß\n",
    "base_model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65066f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏ö‡∏ô validation set\n",
    "predictions = fine_tuned_model.predict(fine_tuned_model.val_dataloader)\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á probability ‚Üí label 0/1\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(predicted_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74375a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9440\n"
     ]
    }
   ],
   "source": [
    "# 1) ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• predict ‡∏ö‡∏ô validation set\n",
    "predictions = fine_tuned_model.predict(fine_tuned_model.val_dataloader)\n",
    "\n",
    "# 2) ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô label 0/1\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 3) ‡∏î‡∏∂‡∏á label ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á validation set\n",
    "true_labels = fine_tuned_model.df_dataset['sentiment_encoded'][-len(predicted_labels):].values\n",
    "\n",
    "# 4) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy\n",
    "val_accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a08765",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_deberta_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_deberta_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_deberta_imdb\\\\spm.model',\n",
       " './fine_tuned_deberta_imdb\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "save_path = \"./fine_tuned_deberta_imdb\"\n",
    "\n",
    "# ‚úÖ save model weights + config\n",
    "fine_tuned_model.model.save_pretrained(save_path)\n",
    "\n",
    "# ‚úÖ save tokenizer ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô fine-tune\n",
    "fine_tuned_model.tokenizer.save_pretrained(save_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10859e1f",
   "metadata": {},
   "source": [
    "# Text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff69653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake\n",
      "Probabilities: [[0.99819607 0.00180395]]\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e069b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely movie using the look and devices of the noir cycle.\n",
      "Probabilities: [[0.00392544 0.99607456]]\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely movie using the look and devices of the noir cycle.\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b0ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors. The story revolves around a ditzy cocktail waitress who becomes famous after inadvertently saving the life of an Arab dignitary. The story goes downhill halfway through the movie and Goldie's charm just doesn't save this movie. Unless you are a Goldie Hawn fan don't go out of your way to see this film.\n",
      "Probabilities: [[0.9945844  0.00541563]]\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors. The story revolves around a ditzy cocktail waitress who becomes famous after inadvertently saving the life of an Arab dignitary. The story goes downhill halfway through the movie and Goldie's charm just doesn't save this movie. Unless you are a Goldie Hawn fan don't go out of your way to see this film.\"\n",
    "\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b43a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Movie: Helen (Kate Capshaw) owns a bookstore in the sleepy, coastal town of Loblolly by the Sea. Divorced, Helen has a young daughter who is going to camp for the summer, giving mother a bit more freedom\n",
      "Probabilities: [[0.24761915 0.75238085]]\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors. The story revolves around a ditzy cocktail waitress who becomes famous after inadvertently saving the life of an Arab dignitary. The story goes downhill halfway through the movie and Goldie's charm just doesn't save this movie. Unless you are a Goldie Hawn fan don't go out of your way to see this film.\"\n",
    "\n",
    "text = \"Helen (Kate Capshaw) owns a bookstore in the sleepy, coastal town of Loblolly by the Sea. Divorced, Helen has a young daughter who is going to camp for the summer, giving mother a bit more freedom\"\n",
    "\n",
    "# tokenize text\n",
    "tokens, masks = fine_tuned_model.tokenize(text)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "dataloader = DataLoader(TensorDataset(tokens, masks), batch_size=1)\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ fine_tuned_model.predict()\n",
    "probs = fine_tuned_model.predict(dataloader)\n",
    "print(\"Review Movie:\", text)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Predicted label:\", np.argmax(probs))  # 1 = positive, 0 = negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3c125",
   "metadata": {},
   "source": [
    "# Import model local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c883e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./fine_tuned_deberta_Lora_imdb\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c1ba5",
   "metadata": {},
   "source": [
    "# Predict Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68c86091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors\n",
      "Predicted: Negative ‚ùå\n"
     ]
    }
   ],
   "source": [
    "text = \"Protocol is an implausible movie whose only saving grace is that it stars Goldie Hawn along with a good cast of supporting actors\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "import torch\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(\"Review:\" , text)\n",
    "print(\"Predicted:\", \"Positive ‚úÖ\" if probs.argmax() == 1 else \"Negative ‚ùå\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î CSV\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "# ‚úÖ tokenize ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "inputs = tokenizer(df[\"review\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# ‚úÖ predict\n",
    "outputs = model(**inputs)\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "pred_labels = probs.argmax(axis=1)\n",
    "\n",
    "# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "df[\"predicted_sentiment\"] = [\"positive\" if p == 1 else \"negative\" for p in pred_labels]\n",
    "\n",
    "# ‚úÖ ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "print(df.head())\n",
    "\n",
    "# ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•\n",
    "df.to_csv(\"reviews_with_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
